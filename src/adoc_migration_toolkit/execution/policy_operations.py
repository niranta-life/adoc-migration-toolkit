"""
Policy operations execution functions.

This module contains execution functions for policy-related operations
including policy export/import, list export, and rule tag export.
"""

import csv
import json
import logging
import os
import tempfile
import threading
from datetime import datetime
from glob import glob
from pathlib import Path

from .utils import create_progress_bar, get_thread_names
from ..shared import globals
from ..shared.file_utils import get_output_file_path


def execute_policy_list_export(client, logger: logging.Logger, quiet_mode: bool = False, verbose_mode: bool = False, existing_target_assets_mode: bool = False):
    """Execute the policy-list-export command.
    
    Args:
        client: API client instance
        logger: Logger instance
        quiet_mode: Whether to suppress console output
        verbose_mode: Whether to enable verbose logging
        existing_target_assets_mode: Whether to filter policies to only include those with assets in merged file
    """
    try:
        # Determine output file path using the policy-export category
        output_file = get_output_file_path("", "policies-all-export.csv", category="policy-export")
        
        if not quiet_mode:
            print(f"\nExporting all rules from ADOC environment")
            if existing_target_assets_mode:
                print(f"üìã EXISTING TARGET ASSETS MODE - Only including policies for assets in merged file")
            print(f"Output will be written to: {output_file}")

            if verbose_mode:
                print("üîä VERBOSE MODE - Detailed output including headers and responses")
            print("="*80)
        
        # If existing target assets mode is enabled, read the merged file first
        existing_asset_ids = set()  # Use set for faster lookups
        if existing_target_assets_mode:
            # Determine the merged file path
            if globals.GLOBAL_OUTPUT_DIR:
                merged_file = globals.GLOBAL_OUTPUT_DIR / "asset-import" / "asset-merged-all.csv"
            else:
                # Look for the most recent adoc-migration-toolkit directory
                current_dir = Path.cwd()
                toolkit_dirs = [d for d in current_dir.iterdir() if d.is_dir() and d.name.startswith("adoc-migration-toolkit-")]
                
                if not toolkit_dirs:
                    error_msg = "No adoc-migration-toolkit directory found. Please run 'transform-and-merge' first to generate the merged file."
                    print(f"‚ùå {error_msg}")
                    logger.error(error_msg)
                    return
                
                toolkit_dirs.sort(key=lambda x: x.stat().st_ctime, reverse=True)
                latest_toolkit_dir = toolkit_dirs[0]
                merged_file = latest_toolkit_dir / "asset-import" / "asset-merged-all.csv"
            
            if not merged_file.exists():
                error_msg = f"Merged file not found: {merged_file}"
                print(f"‚ùå {error_msg}")
                print("üí° Please run 'transform-and-merge' first to generate the asset-merged-all.csv file")
                logger.error(error_msg)
                return
            
            if not quiet_mode:
                print(f"üìÑ Reading existing target assets from: {merged_file}")
            
            # Read the merged file to get existing asset IDs
            with open(merged_file, 'r', newline='', encoding='utf-8') as f:
                reader = csv.DictReader(f)
                for row in reader:
                    source_id = row.get('source_id')
                    if source_id:
                        existing_asset_ids.add(source_id)
            
            if not existing_asset_ids:
                error_msg = "No asset IDs found in merged file"
                print(f"‚ùå {error_msg}")
                logger.error(error_msg)
                return
            
            if not quiet_mode:
                print(f"üìä Found {len(existing_asset_ids)} existing target assets")
                if verbose_mode:
                    print(f"üìã Sample asset IDs: {list(existing_asset_ids)[:5]}")
        
        # Step 1: Get total count of policies
        if not quiet_mode:
            print("Getting total rules count...")
        
        if verbose_mode:
            print("\nGET Request Headers:")
            print(f"  Endpoint: /catalog-server/api/rules?page=0&size=0&ruleStatus=ENABLED")
            print(f"  Method: GET")
            print(f"  Content-Type: application/json")
            print(f"  Authorization: Bearer [REDACTED]")
            if hasattr(client, 'tenant') and client.tenant:
                print(f"  X-Tenant: {client.tenant}")
        
        count_response = client.make_api_call(
            endpoint="/catalog-server/api/rules?page=0&size=0&ruleStatus=ENABLED",
            method='GET'
        )
        
        if verbose_mode:
            print("\nCount Response:")
            print(json.dumps(count_response, indent=2, ensure_ascii=False))
        
        # Extract total count
        if not count_response or 'meta' not in count_response or 'count' not in count_response['meta']:
            error_msg = "Failed to get total rules count from response"
            print(f"‚ùå {error_msg}")
            logger.error(error_msg)
            return
        
        total_count = count_response['meta']['count']
        page_size = 1000  # Default page size
        total_pages = (total_count + page_size - 1) // page_size  # Ceiling division
        
        if not quiet_mode:
            print(f"Total rules found: {total_count}")
            print(f"Page size: {page_size}")
            print(f"Total pages to retrieve: {total_pages}")
            print("="*80)
        
        # Step 2: Retrieve all pages and collect policies
        all_policies = []
        successful_pages = 0
        failed_pages = 0
        
        for page in range(total_pages):
            if not quiet_mode:
                print(f"\n[Page {page + 1}/{total_pages}] Retrieving rules...")
            
            try:
                if verbose_mode:
                    print(f"\nGET Request Headers:")
                    print(f"  Endpoint: /catalog-server/api/rules?page={page}&size={page_size}&ruleStatus=ENABLED")
                    print(f"  Method: GET")
                    print(f"  Content-Type: application/json")
                    print(f"  Authorization: Bearer [REDACTED]")
                    if hasattr(client, 'tenant') and client.tenant:
                        print(f"  X-Tenant: {client.tenant}")
                
                page_response = client.make_api_call(
                    endpoint=f"/catalog-server/api/rules?page={page}&size={page_size}&ruleStatus=ENABLED",
                    method='GET'
                )
                
                if verbose_mode:
                    print(f"\nPage {page + 1} Response:")
                    print(json.dumps(page_response, indent=2, ensure_ascii=False))
                
                # Extract policies from response
                if page_response and 'rules' in page_response:
                    page_policies = page_response['rules']
                    # Extract the actual policy objects from the nested structure
                    actual_policies = []
                    for policy_wrapper in page_policies:
                        if 'rule' in policy_wrapper:
                            actual_policies.append(policy_wrapper['rule'])
                        else:
                            # Fallback: if no 'rule' wrapper, use the object directly
                            actual_policies.append(policy_wrapper)
                    
                    all_policies.extend(actual_policies)
                    
                    if not quiet_mode:
                        print(f"‚úÖ Page {page + 1}: Retrieved {len(actual_policies)} rules")
                    else:
                        print(f"‚úÖ Page {page + 1}/{total_pages}: {len(actual_policies)} rules")
                    
                    successful_pages += 1
                else:
                    error_msg = f"Invalid response format for page {page + 1} - no rules found"
                    if not quiet_mode:
                        print(f"‚ùå {error_msg}")
                    logger.error(error_msg)
                    failed_pages += 1
                    
            except Exception as e:
                error_msg = f"Failed to retrieve page {page + 1}: {e}"
                if not quiet_mode:
                    print(f"‚ùå {error_msg}")
                logger.error(error_msg)
                failed_pages += 1
        
        # Step 3: Process each policy to get asset details
        if not quiet_mode:
            print(f"\nProcessing {len(all_policies)} rules to extract asset information...")
        
        processed_policies = []
        total_asset_calls = 0
        successful_asset_calls = 0
        failed_asset_calls = 0
        failed_rules = []  # Track rules that failed to retrieve assemblies
        excluded_policies = 0  # Track policies excluded due to missing assets
        
        # Create progress bar using tqdm utility
        progress_bar = create_progress_bar(
            total=len(all_policies),
            desc="Processing rule",
            unit="rules",
            disable=verbose_mode
        )
        
        for i, policy in enumerate(all_policies, 1):
            # Update progress bar with current policy ID using set_postfix
            progress_bar.set_postfix(rule_id=policy.get('id', 'unknown'))
            
            # Extract tableAssetIds from backingAssets for this policy
            table_asset_ids = []
            backing_assets = policy.get('backingAssets', [])
            for asset in backing_assets:
                table_asset_id = asset.get('tableAssetId')
                if table_asset_id:
                    table_asset_ids.append(table_asset_id)
            
            # Check if policy should be processed based on existing target assets
            should_process_policy = True
            if existing_target_assets_mode and table_asset_ids:
                # Check if any of the policy's tableAssetIds exist in the existing asset IDs
                policy_has_existing_assets = False
                matching_assets = []
                for table_asset_id in table_asset_ids:
                    if str(table_asset_id) in existing_asset_ids:
                        policy_has_existing_assets = True
                        matching_assets.append(str(table_asset_id))
                
                should_process_policy = policy_has_existing_assets
                
                if verbose_mode:
                    if policy_has_existing_assets:
                        print(f"‚úÖ Processing policy {policy.get('id')}: Found matching assets {matching_assets}")
                    else:
                        print(f"‚ùå Skipping policy {policy.get('id')}: No matching assets in target environment (assets: {table_asset_ids})")
                
                if not policy_has_existing_assets:
                    excluded_policies += 1
                    # Update progress bar and continue to next policy
                    progress_bar.update(1)
                    continue
            
            # Get asset details for this policy's tableAssetIds
            asset_details = {}
            assembly_details = {}
            
            if table_asset_ids:
                # Convert to comma-separated string for this policy
                table_asset_ids_str = ','.join(map(str, table_asset_ids))
                total_asset_calls += 1
                
                if verbose_mode:
                    print(f"\nGET Request Headers:")
                    print(f"  Endpoint: /catalog-server/api/assets/search?ids={table_asset_ids_str}")
                    print(f"  Method: GET")
                    print(f"  Content-Type: application/json")
                    print(f"  Authorization: Bearer [REDACTED]")
                    if hasattr(client, 'tenant') and client.tenant:
                        print(f"  X-Tenant: {client.tenant}")
                
                try:
                    assets_response = client.make_api_call(
                        endpoint=f"/catalog-server/api/assets/search?ids={table_asset_ids_str}",
                        method='GET'
                    )
                    
                    if verbose_mode:
                        print("\nAssets Response:")
                        print(json.dumps(assets_response, indent=2, ensure_ascii=False))
                    
                    # Extract asset details for this policy
                    if assets_response and 'assets' in assets_response:
                        for asset in assets_response['assets']:
                            asset_id = asset.get('id')
                            if asset_id:
                                asset_details[asset_id] = asset
                    
                    # Extract assembly details for this policy
                    if assets_response and 'assemblies' in assets_response:
                        for assembly in assets_response['assemblies']:
                            assembly_id = assembly.get('id')
                            if assembly_id:
                                assembly_details[assembly_id] = assembly
                    
                    successful_asset_calls += 1
                    if verbose_mode:
                        print(f"‚úÖ Retrieved details for {len(asset_details)} assets and {len(assembly_details)} assemblies for policy {policy.get('id')}")
                        
                except Exception as e:
                    error_msg = f"Failed to retrieve asset details for policy {policy.get('id')}: {e}"
                    if verbose_mode:
                        print(f"‚ùå {error_msg}")
                    logger.error(error_msg)
                    failed_asset_calls += 1
                    
                    # Track failed rule
                    failed_rules.append({
                        'policy_id': policy.get('id', 'unknown'),
                        'policy_type': policy.get('type', 'unknown'),
                        'error': str(e),
                        'table_asset_ids': table_asset_ids
                    })
            
            # Add asset and assembly details to the policy for later processing
            policy['_asset_details'] = asset_details
            policy['_assembly_details'] = assembly_details
            processed_policies.append(policy)
            
            # Update progress bar
            progress_bar.update(1)
        
        # Close progress bar
        progress_bar.close()
        
        # Show completion summary
        if not quiet_mode:
            print(f"\nAsset API calls completed:")
            print(f"  Total API calls made: {total_asset_calls}")
            print(f"  Successful calls: {successful_asset_calls}")
            print(f"  Failed calls: {failed_asset_calls}")
            
            # Show failed rules summary
            if failed_rules:
                print(f"\n‚ùå Failed Rules Summary ({len(failed_rules)} rules):")
                print("="*80)
                for failed_rule in failed_rules:
                    print(f"Rule ID: {failed_rule['policy_id']}")
                    print(f"Rule Type: {failed_rule['policy_type']}")
                    print(f"Table Asset IDs: {', '.join(map(str, failed_rule['table_asset_ids']))}")
                    print(f"Error: {failed_rule['error']}")
                    print("-" * 40)
                print("="*80)
        
        # Step 4: Write policies to CSV file with additional columns
        if not quiet_mode:
            print(f"\nWriting {len(processed_policies)} rules to CSV file...")
        
        # Create output directory if needed
        output_file.parent.mkdir(parents=True, exist_ok=True)
        
        with open(output_file, 'w', newline='', encoding='utf-8') as f:
            writer = csv.writer(f, quoting=csv.QUOTE_ALL)
            
            # Write header with additional columns
            writer.writerow(['id', 'type', 'engineType', 'tableAssetIds', 'assemblyIds', 'assemblyNames', 'sourceTypes'])
            
            # Write policy data
            for policy in processed_policies:
                policy_id = policy.get('id', '')
                policy_type = policy.get('type', '') or ''  # Convert None to empty string
                engine_type = policy.get('engineType', '') or ''  # Convert None to empty string
                
                # Extract tableAssetIds from backingAssets
                table_asset_ids = []
                assembly_ids = set()
                assembly_names = set()
                source_types = set()
                
                backing_assets = policy.get('backingAssets', [])
                asset_details = policy.get('_asset_details', {})
                assembly_details = policy.get('_assembly_details', {})
                
                for asset in backing_assets:
                    table_asset_id = asset.get('tableAssetId')
                    if table_asset_id:
                        table_asset_ids.append(str(table_asset_id))
                        
                        # Get assembly information from asset details
                        if table_asset_id in asset_details:
                            asset_detail = asset_details[table_asset_id]
                            assembly_id = asset_detail.get('assemblyId')
                            if assembly_id and assembly_id in assembly_details:
                                assembly = assembly_details[assembly_id]
                                assembly_ids.add(str(assembly_id))
                                assembly_name = assembly.get('name', '')
                                if assembly_name:
                                    assembly_names.add(assembly_name)
                                source_type = assembly.get('sourceType', {}).get('name', '')
                                if source_type:
                                    source_types.add(source_type)
                
                # Convert sets to comma-separated strings
                table_asset_ids_str = ','.join(table_asset_ids)
                assembly_ids_str = ','.join(sorted(assembly_ids))
                assembly_names_str = ','.join(sorted(assembly_names))
                source_types_str = ','.join(sorted(source_types))
                
                # Write the policy to CSV (no additional filtering since policies are already filtered)
                writer.writerow([
                    policy_id, 
                    policy_type, 
                    engine_type, 
                    table_asset_ids_str,
                    assembly_ids_str,
                    assembly_names_str,
                    source_types_str
                ])
        
        # Step 5: Sort the CSV file by id
        if not quiet_mode:
            print("Sorting CSV file by id...")
        
        # Read all rows
        rows = []
        with open(output_file, 'r', newline='', encoding='utf-8') as f:
            reader = csv.reader(f)
            header = next(reader)  # Skip header
            rows = list(reader)
        
        # Sort rows by id
        def sort_key(row):
            policy_id = row[0] if len(row) > 0 else ''
            # Convert policy_id to int for proper numeric sorting, fallback to string
            try:
                policy_id_int = int(policy_id) if policy_id else 0
            except (ValueError, TypeError):
                policy_id_int = 0
            return policy_id_int
        
        rows.sort(key=sort_key)
        
        # Write sorted data back to file
        with open(output_file, 'w', newline='', encoding='utf-8') as f:
            writer = csv.writer(f, quoting=csv.QUOTE_ALL)
            writer.writerow(header)
            writer.writerows(rows)
        
        # Step 6: Print statistics
        if not quiet_mode:
            print("\n" + "="*80)
            print("RULES LIST EXPORT SUMMARY")
            print("="*80)
            print(f"Output file: {output_file}")
            print(f"Total rules exported: {len(processed_policies)}")
            if existing_target_assets_mode:
                print(f"Policies excluded (no matching assets): {excluded_policies}")
            print(f"Successful pages: {successful_pages}")
            print(f"Failed pages: {failed_pages}")
            print(f"Total pages processed: {total_pages}")
            print(f"Total table assets found: {total_asset_calls}")
            print(f"Total asset API calls made: {total_asset_calls}")
            
            # Calculate total assemblies found
            total_assemblies = 0
            for policy in processed_policies:
                total_assemblies += len(policy.get('_assembly_details', {}))
            print(f"Total assemblies found: {total_assemblies}")
            
            # Additional statistics
            type_counts = {}
            engine_type_counts = {}
            assembly_name_counts = {}
            source_type_counts = {}
            
            for policy in processed_policies:
                policy_type = policy.get('type', '') or 'UNKNOWN'
                engine_type = policy.get('engineType', '') or 'UNKNOWN'
                
                type_counts[policy_type] = type_counts.get(policy_type, 0) + 1
                engine_type_counts[engine_type] = engine_type_counts.get(engine_type, 0) + 1
                
                # Extract assembly names and source types from backingAssets
                backing_assets = policy.get('backingAssets', [])
                asset_details = policy.get('_asset_details', {})
                assembly_details = policy.get('_assembly_details', {})
                
                for asset in backing_assets:
                    table_asset_id = asset.get('tableAssetId')
                    if table_asset_id and table_asset_id in asset_details:
                        asset_detail = asset_details[table_asset_id]
                        assembly_id = asset_detail.get('assemblyId')
                        if assembly_id and assembly_id in assembly_details:
                            assembly = assembly_details[assembly_id]
                            assembly_name = assembly.get('name', '')
                            if assembly_name:
                                assembly_name_counts[assembly_name] = assembly_name_counts.get(assembly_name, 0) + 1
                            source_type = assembly.get('sourceType', {}).get('name', '')
                            if source_type:
                                source_type_counts[source_type] = source_type_counts.get(source_type, 0) + 1
            
            print(f"\nüìä DETAILED STATISTICS SUMMARY")
            print("-" * 50)
            print(f"Total rules processed: {len(processed_policies)}")
            print(f"Total asset API calls: {total_asset_calls}")
            print(f"Successful asset calls: {successful_asset_calls}")
            print(f"Failed asset calls: {failed_asset_calls}")
            print(f"Total assemblies found: {total_assemblies}")
            
            # Calculate success rate
            if total_asset_calls > 0:
                success_rate = (successful_asset_calls / total_asset_calls) * 100
                print(f"Asset API success rate: {success_rate:.1f}%")
            
            # Rule Type Statistics
            if type_counts:
                print(f"\nüîß RULE TYPES ({len(type_counts)} types):")
                print("-" * 50)
                total_rules = len(processed_policies)
                for rule_type, count in sorted(type_counts.items(), key=lambda x: x[1], reverse=True):
                    percentage = (count / total_rules) * 100
                    print(f"  {rule_type:<20} {count:>5} rules ({percentage:>5.1f}%)")
            
            # Engine Type Statistics
            if engine_type_counts:
                print(f"\n‚öôÔ∏è  ENGINE TYPES ({len(engine_type_counts)} types):")
                print("-" * 50)
                for engine_type, count in sorted(engine_type_counts.items(), key=lambda x: x[1], reverse=True):
                    percentage = (count / len(processed_policies)) * 100
                    print(f"  {engine_type:<20} {count:>5} rules ({percentage:>5.1f}%)")
            
            # Assembly Statistics
            if assembly_name_counts:
                print(f"\nüèóÔ∏è  ASSEMBLIES ({len(assembly_name_counts)} assemblies):")
                print("-" * 50)
                total_assembly_rules = sum(assembly_name_counts.values())
                for assembly_name, count in sorted(assembly_name_counts.items(), key=lambda x: x[1], reverse=True):
                    percentage = (count / total_assembly_rules) * 100
                    print(f"  {assembly_name:<30} {count:>5} rules ({percentage:>5.1f}%)")
            else:
                print(f"\nüèóÔ∏è  ASSEMBLIES: No assemblies found")
            
            # Source Type Statistics
            if source_type_counts:
                print(f"\nüì° SOURCE TYPES ({len(source_type_counts)} types):")
                print("-" * 50)
                total_source_rules = sum(source_type_counts.values())
                for source_type, count in sorted(source_type_counts.items(), key=lambda x: x[1], reverse=True):
                    percentage = (count / total_source_rules) * 100
                    print(f"  {source_type:<20} {count:>5} rules ({percentage:>5.1f}%)")
            else:
                print(f"\nüì° SOURCE TYPES: No source types found")
            
            # Failed Rules Summary (if any)
            if failed_rules:
                print(f"\n‚ùå FAILED RULES SUMMARY ({len(failed_rules)} rules):")
                print("="*80)
                for failed_rule in failed_rules:
                    print(f"Policy ID: {failed_rule['policy_id']}")
                    print(f"Policy Type: {failed_rule['policy_type']}")
                    print(f"Table Asset IDs: {', '.join(map(str, failed_rule['table_asset_ids']))}")
                    print(f"Error: {failed_rule['error']}")
                    print("-" * 40)
                print("="*80)
            
            print("="*80)
        else:
            print(f"‚úÖ Policies list export completed: {len(processed_policies)} policies exported to {output_file}")
        
        # After all threads complete, aggregate excluded_policies_and_table_assets_mapping and write to CSV
        excluded_policies_all = []
        for thread_result in thread_results:
            if 'excluded_policies_and_table_assets_mapping' in thread_result:
                excluded_policies_all.extend(thread_result['excluded_policies_and_table_assets_mapping'])
        if excluded_policies_all:
            # Determine output directory (reuse logic from other exports)
            if globals.GLOBAL_OUTPUT_DIR:
                output_dir = globals.GLOBAL_OUTPUT_DIR / "policy-export"
            else:
                current_dir = Path.cwd()
                toolkit_dirs = [d for d in current_dir.iterdir() if d.is_dir() and d.name.startswith("adoc-migration-toolkit-")]
                if toolkit_dirs:
                    toolkit_dirs.sort(key=lambda x: x.stat().st_ctime, reverse=True)
                    latest_toolkit_dir = toolkit_dirs[0]
                    output_dir = latest_toolkit_dir / "policy-export"
                else:
                    output_dir = Path("policy-export")
            output_dir.mkdir(parents=True, exist_ok=True)
            excluded_file = output_dir / "excluded_sql_view_policies.csv"
            with open(excluded_file, 'w', newline='', encoding='utf-8') as f:
                writer = csv.writer(f, quoting=csv.QUOTE_ALL)
                writer.writerow(["asset_id", "policy_id", "policy_name", "source_asset_uid"])
                for asset_id, policy_id, policy_name, source_asset_uid in excluded_policies_all:
                    writer.writerow([asset_id, policy_id, policy_name, source_asset_uid])
            if not quiet_mode:
                print(f"\nExcluded policies written to: {excluded_file}")
        
    except Exception as e:
        error_msg = f"Error in policy-list-export: {e}"
        if not quiet_mode:
            print(f"‚ùå {error_msg}")
        logger.error(error_msg)


def execute_policy_list_export_parallel(client, logger: logging.Logger, quiet_mode: bool = False, verbose_mode: bool = False, existing_target_assets_mode: bool = False, max_threads: int = 5):
    """Execute the policy-list-export command with parallel processing.
    
    Args:
        client: API client instance
        logger: Logger instance
        quiet_mode: Whether to suppress console output
        verbose_mode: Whether to enable verbose logging
        existing_target_assets_mode: Whether to filter policies to only include those with assets in merged file
        max_threads: Maximum number of threads to use (default: 5)
    """
    try:
        # Determine output file path using the policy-export category
        output_file = get_output_file_path("", "policies-all-export.csv", category="policy-export")
        sql_policies_output_file = get_output_file_path("", "policies-sql-export.csv", category="policy-export")
        sql_view_policies_output_file = get_output_file_path("", "policies-sql-view-export.csv", category="policy-export")
        if not quiet_mode:
            print(f"\nExporting all rules from ADOC environment (Parallel Mode)")
            if existing_target_assets_mode:
                print(f"üìã EXISTING TARGET ASSETS MODE - Only including policies for assets in merged file")
            print(f"Output will be written to: {output_file}")
            if verbose_mode:
                print("üîä VERBOSE MODE - Detailed output including headers and responses")
            print("="*80)
        
        # If existing target assets mode is enabled, read the merged file first
        existing_asset_ids = set()  # Use set for faster lookups
        existing_asset_ids_sql_views = set()  # Use set for faster lookups
        source_id_to_uid_sql_views = {}  # Map source_id to source_uid for SQL views
        if existing_target_assets_mode:
            # Determine the merged file path
            if globals.GLOBAL_OUTPUT_DIR:
                merged_file = globals.GLOBAL_OUTPUT_DIR / "asset-import" / "asset-merged-all.csv"
                merged_file_sql_views = globals.GLOBAL_OUTPUT_DIR / "asset-import" / "asset-merged-all_sql_views.csv"
            else:
                # Look for the most recent adoc-migration-toolkit directory
                current_dir = Path.cwd()
                toolkit_dirs = [d for d in current_dir.iterdir() if d.is_dir() and d.name.startswith("adoc-migration-toolkit-")]
                
                if not toolkit_dirs:
                    error_msg = "No adoc-migration-toolkit directory found. Please run 'transform-and-merge' first to generate the merged file."
                    print(f"‚ùå {error_msg}")
                    logger.error(error_msg)
                    return
                
                toolkit_dirs.sort(key=lambda x: x.stat().st_ctime, reverse=True)
                latest_toolkit_dir = toolkit_dirs[0]
                merged_file = latest_toolkit_dir / "asset-import" / "asset-merged-all.csv"
                merged_file_sql_views = latest_toolkit_dir / "asset-import" / "asset-merged-all_sql_views.csv"
            print(f"merged_file_sql_views: {merged_file_sql_views}")
            if not merged_file.exists():
                error_msg = f"Merged file not found: {merged_file}"
                print(f"‚ùå {error_msg}")
                print("üí° Please run 'transform-and-merge' first to generate the asset-merged-all.csv file")
                logger.error(error_msg)
                return
            
            if not quiet_mode:
                print(f"üìÑ Reading existing target assets from: {merged_file}")
            
            # Read the merged file to get existing asset IDs
            with open(merged_file, 'r', newline='', encoding='utf-8') as f:
                reader = csv.DictReader(f)
                for row in reader:
                    source_id = row.get('source_id')
                    if source_id:
                        existing_asset_ids.add(source_id)

            if merged_file_sql_views.exists():
                with open(merged_file_sql_views, 'r', newline='', encoding='utf-8') as f:
                    reader = csv.DictReader(f)
                    for row in reader:
                        source_id = row.get('source_id')
                        source_uid = row.get('source_uid')
                        if source_id:
                            existing_asset_ids_sql_views.add(source_id)
                            if source_uid:
                                source_id_to_uid_sql_views[source_id] = source_uid
            
            if not existing_asset_ids:
                error_msg = "No asset IDs found in merged file"
                print(f"‚ùå {error_msg}")
                logger.error(error_msg)
                return
            
            if not quiet_mode:
                print(f"üìä Found {len(existing_asset_ids)} existing target assets")
                if verbose_mode:
                    print(f"üìã Sample asset IDs: {list(existing_asset_ids)[:5]}")
        
        # Step 1: Get total count of policies
        if not quiet_mode:
            print("Getting total rules count...")
        
        count_response = client.make_api_call(
            endpoint="/catalog-server/api/rules?page=0&size=0&ruleStatus=ENABLED",
            method='GET'
        )
        
        if not count_response or 'meta' not in count_response or 'count' not in count_response['meta']:
            error_msg = "Failed to get total rules count from response"
            print(f"‚ùå {error_msg}")
            logger.error(error_msg)
            return
        
        total_count = count_response['meta']['count']
        
        if not quiet_mode:
            print(f"Total rules found: {total_count}")
        
        # Step 2: Get all policies first (sequential - this is fast)
        if not quiet_mode:
            print("Retrieving all policies...")
        
        all_policies = []
        page_size = 1000
        total_pages = (total_count + page_size - 1) // page_size
        
        for page in range(total_pages):
            if not quiet_mode:
                print(f"  Retrieving page {page + 1}/{total_pages}...")
            
            page_response = client.make_api_call(
                    endpoint=f"/catalog-server/api/rules?page={page}&size={page_size}&ruleStatus=ENABLED",
                    method='GET'
                )
            
            if page_response and 'rules' in page_response:
                page_policies = page_response['rules']
                actual_policies = []
                for policy_wrapper in page_policies:
                    if 'rule' in policy_wrapper:
                        actual_policies.append(policy_wrapper['rule'])
                    else:
                        actual_policies.append(policy_wrapper)
                
                all_policies.extend(actual_policies)
        
        if not quiet_mode:
            print(f"Retrieved {len(all_policies)} policies")
        
        # Step 3: Calculate thread configuration for asset processing
        min_policies_per_thread = 10
        if len(all_policies) < min_policies_per_thread:
            num_threads = 1
            policies_per_thread = len(all_policies)
        else:
            num_threads = min(max_threads, (len(all_policies) + min_policies_per_thread - 1) // min_policies_per_thread)
            policies_per_thread = (len(all_policies) + num_threads - 1) // num_threads
        
        if not quiet_mode:
            print(f"Using {num_threads} threads to process {len(all_policies)} policies")
            print(f"Policies per thread: {policies_per_thread}")
            print("="*80)
        
        # Step 4: Process asset details in parallel
        temp_files = []
        thread_results = []
        
        def process_policy_chunk(thread_id, start_index, end_index):
            """Process a chunk of policies for a specific thread."""
            # Create a thread-local client instance
            thread_client = type(client)(
                host=client.host,
                access_key=client.access_key,
                secret_key=client.secret_key,
                tenant=getattr(client, 'tenant', None)
            )
            
            # Create temporary file for this thread
            temp_file = tempfile.NamedTemporaryFile(mode='w', delete=False, suffix='.csv', encoding='utf-8')
            temp_files.append(temp_file.name)
            
            # Get policies for this thread
            thread_policies = all_policies[start_index:end_index]
            thread_names = get_thread_names()
            thread_name = thread_names[thread_id] if thread_id < len(thread_names) else f"Thread {thread_id}"
            # Create progress bar for this thread using utility function
            progress_bar = create_progress_bar(
                total=len(thread_policies),
                desc= thread_name,
                unit="policies",
                disable=quiet_mode,
                position=thread_id,
                leave=False
            )
            
            processed_policies = []
            total_asset_calls = 0
            successful_asset_calls = 0
            failed_asset_calls = 0
            excluded_policies = 0  # Track policies excluded due to missing assets
            excluded_policies_and_table_assets_mapping = []
            
            # Process each policy in this thread's range
            for policy in thread_policies:
                # Extract tableAssetIds from backingAssets
                table_asset_ids = []
                backing_assets = policy.get('backingAssets', [])
                for asset in backing_assets:
                    table_asset_id = asset.get('tableAssetId')
                    if table_asset_id:
                        table_asset_ids.append(table_asset_id)
                
                # Check if policy should be processed based on existing target assets
                should_process_policy = True
                if existing_target_assets_mode and table_asset_ids:
                    # Check if any of the policy's tableAssetIds exist in the existing asset IDs
                    policy_has_existing_assets = False
                    matching_assets = []
                    for table_asset_id in table_asset_ids:
                        if str(table_asset_id) in existing_asset_ids:
                            policy_has_existing_assets = True
                            matching_assets.append(str(table_asset_id))
                        elif str(table_asset_id) in existing_asset_ids_sql_views:
                            policy_has_existing_assets = True
                            source_asset_uid = source_id_to_uid_sql_views.get(str(table_asset_id), "")
                            excluded_policies_and_table_assets_mapping.append((table_asset_id, source_asset_uid, policy.get("id"), policy.get("name")))
                            
                    
                    should_process_policy = policy_has_existing_assets
                    
                    if verbose_mode:
                        if policy_has_existing_assets:
                            print(f"‚úÖ Thread {thread_name}: Processing policy {policy.get('id')}: Found matching assets {matching_assets}")
                        else:
                            print(f"‚ùå Thread {thread_name}: Skipping policy {policy.get('id')}: No matching assets in target environment (assets: {table_asset_ids})")
                    
                    if not policy_has_existing_assets:
                        excluded_policies += 1
                        # Update progress bar and continue to next policy
                        progress_bar.update(1)
                        continue
                
                # Get asset details for this policy's tableAssetIds
                asset_details = {}
                asset_details_types = {}
                assembly_details = {}
                
                if table_asset_ids:
                    table_asset_ids_str = ','.join(map(str, table_asset_ids))
                    total_asset_calls += 1
                    
                    try:
                        assets_response = thread_client.make_api_call(
                            endpoint=f"/catalog-server/api/assets/search?ids={table_asset_ids_str}",
                            method='GET'
                        )

                        if assets_response and 'assets' in assets_response:
                            for asset in assets_response['assets']:
                                asset_id = asset.get('id')
                                if asset_id:
                                    asset_details[asset_id] = asset
                                asset_type_definition = asset.get('assetType')
                                if asset_type_definition:
                                    asset_id_type = asset_type_definition.get('name')
                                    asset_details_types[asset_id] = asset_id_type
                        
                        if assets_response and 'assemblies' in assets_response:
                            for assembly in assets_response['assemblies']:
                                assembly_id = assembly.get('id')
                                if assembly_id:
                                    assembly_details[assembly_id] = assembly
                        
                        successful_asset_calls += 1
                        
                    except Exception as e:
                        failed_asset_calls += 1
                        logger.error(f"Thread {thread_name}: Failed to retrieve asset details for policy {policy.get('id')}: {e}")

                # Add asset and assembly details to the policy
                policy['_asset_details'] = asset_details
                policy['_assembly_details'] = assembly_details
                policy['_asset_details_types'] = asset_details_types
                processed_policies.append(policy)
                
                # Update progress bar
                progress_bar.update(1)
            
            progress_bar.close()
            # Write processed policies to temporary CSV file
            with open(temp_file.name, 'w', newline='', encoding='utf-8') as f:
                writer = csv.writer(f, quoting=csv.QUOTE_ALL)
                writer.writerow(['id', 'type', 'engineType', 'tableAssetIds', 'assemblyIds', 'assemblyNames', 'sourceTypes', 'subType','policyName'])
                sqlBasedPolicies = 0
                columnBasedPolicies = 0
                for policy in processed_policies:
                    policy_id = policy.get('id', '')
                    policy_type = policy.get('type', '') or ''
                    policy_sub_type = policy.get('subType', '') or ''
                    engine_type = policy.get('engineType', '') or ''
                    policy_name = policy.get('name', '')
                    if policy_type is not None and policy_sub_type == 'SQL':
                        sqlBasedPolicies += 1
                    else:
                        columnBasedPolicies += 1
                    # Extract tableAssetIds from backingAssets
                    table_asset_ids = []
                    table_asset_ids_type = []
                    assembly_ids = set()
                    assembly_names = set()
                    source_types = set()
                    
                    backing_assets = policy.get('backingAssets', [])
                    asset_details = policy.get('_asset_details', {})
                    asset_details_types = policy.get('_asset_details_types', {})
                    assembly_details = policy.get('_assembly_details', {})
                    
                    for asset in backing_assets:
                        table_asset_id = asset.get('tableAssetId')
                        if table_asset_id:
                            table_asset_ids.append(str(table_asset_id))
                            table_asset_ids_type.append(asset_details_types[table_asset_id])
                            # Get assembly information from asset details
                            if table_asset_id in asset_details:
                                asset_detail = asset_details[table_asset_id]
                                assembly_id = asset_detail.get('assemblyId')
                                if assembly_id and assembly_id in assembly_details:
                                    assembly = assembly_details[assembly_id]
                                    assembly_ids.add(str(assembly_id))
                                    assembly_name = assembly.get('name', '')
                                    if assembly_name:
                                        assembly_names.add(assembly_name)
                                    source_type = assembly.get('sourceType', {}).get('name', '')
                                    if source_type:
                                        source_types.add(source_type)
                    
                    # Convert sets to comma-separated strings
                    table_asset_ids_str = ','.join(table_asset_ids)
                    table_asset_ids_type_str = ','.join(table_asset_ids_type)
                    assembly_ids_str = ','.join(sorted(assembly_ids))
                    assembly_names_str = ','.join(sorted(assembly_names))
                    source_types_str = ','.join(sorted(source_types))
                    
                    writer.writerow([
                        policy_id, 
                        policy_type,
                        engine_type, 
                        table_asset_ids_str,
                        assembly_ids_str,
                        assembly_names_str,
                        source_types_str,
                        policy_sub_type,
                        policy_name,
                        table_asset_ids_type_str
                    ])
            
            return {
                'thread_id': thread_id,
                'processed': len(processed_policies),
                'excluded': excluded_policies,
                'total_asset_calls': total_asset_calls,
                'successful_asset_calls': successful_asset_calls,
                'failed_asset_calls': failed_asset_calls,
                'temp_file': temp_file.name,
                'sqlBasedPolicies': sqlBasedPolicies,
                'columnBasedPolicies': columnBasedPolicies,
                'excluded_policies_and_table_assets_mapping': excluded_policies_and_table_assets_mapping
            }
        
        # Step 5: Start threads
        threads = []
        for i in range(num_threads):
            start_index = i * policies_per_thread
            end_index = min(start_index + policies_per_thread, len(all_policies))
            
            thread = threading.Thread(
                target=lambda tid=i, start=start_index, end=end_index: thread_results.append(
                    process_policy_chunk(tid, start, end)
                )
            )
            threads.append(thread)
            thread.start()
        
        # Step 6: Wait for all threads to complete
        for thread in threads:
            thread.join()
        
        # Step 7: Merge temporary files
        if not quiet_mode:
            print("\nMerging temporary files...")
        
        # Create output directory if needed
        output_file.parent.mkdir(parents=True, exist_ok=True)
        
        with open(output_file, 'w', newline='', encoding='utf-8') as output_csv:
            writer = csv.writer(output_csv, quoting=csv.QUOTE_ALL)
            
            # Write header
            writer.writerow(['id', 'type', 'engineType', 'tableAssetIds', 'assemblyIds', 'assemblyNames', 'sourceTypes', 'subType', 'policyName', 'tableAssetIdsTypes'])
            
            # Merge all temporary files
            for temp_file in temp_files:
                try:
                    with open(temp_file, 'r', newline='', encoding='utf-8') as temp_csv:
                        reader = csv.reader(temp_csv)
                        next(reader)  # Skip header
                        for row in reader:
                            writer.writerow(row)
                except Exception as e:
                    logger.error(f"Error reading temporary file {temp_file}: {e}")
                finally:
                    # Clean up temporary file
                    try:
                        os.unlink(temp_file)
                    except Exception as e:
                        logger.warning(f"Could not delete temporary file {temp_file}: {e}")
        
        # Step 8: Sort the CSV file by id
        if not quiet_mode:
            print("Sorting CSV file by id...")
        
        # Read all rows
        rows = []
        with open(output_file, 'r', newline='', encoding='utf-8') as f:
            reader = csv.reader(f)
            header = next(reader)  # Skip header
            rows = list(reader)
        
        # Sort rows by id
        def sort_key(row):
            policy_id = row[0] if len(row) > 0 else ''
            # Convert policy_id to int for proper numeric sorting, fallback to string
            try:
                policy_id_int = int(policy_id) if policy_id else 0
            except (ValueError, TypeError):
                policy_id_int = 0
            return policy_id_int
        
        rows.sort(key=sort_key)
        
        # Write sorted data back to file
        with open(output_file, 'w', newline='', encoding='utf-8') as f:
            writer = csv.writer(f, quoting=csv.QUOTE_ALL)
            writer.writerow(header)
            writer.writerows(rows)
        # Write SQL based policies to separate file
        with open(sql_policies_output_file, 'w', newline='', encoding='utf-8') as f:
            writer = csv.writer(f, quoting=csv.QUOTE_ALL)
            writer.writerow(header)
            for row in rows:
                if row[7] == 'SQL':
                    writer.writerow(row)
        print(f"SQL based policies exported to {sql_policies_output_file}")
        # Write SQL View based policies to separate file
        with open(sql_view_policies_output_file, 'w', newline='', encoding='utf-8') as f:
            writer = csv.writer(f, quoting=csv.QUOTE_ALL)
            writer.writerow(header)
            for row in rows:
                if any(val.strip() == 'SQL_VIEW' for val in row[9].split(',')):
                    writer.writerow(row)
        print(f"SQL based policies exported to {sql_view_policies_output_file}")
        # Step 9: Print statistics
        if not quiet_mode:
            print("\n" + "="*80)
            print("RULES LIST EXPORT SUMMARY (PARALLEL MODE)")
            print("="*80)
            print(f"Output file: {output_file}")
            
            total_processed = 0
            total_excluded = 0
            total_asset_calls = 0
            total_successful_asset_calls = 0
            total_failed_asset_calls = 0
            total_sql_based_policies = 0
            total_column_based_policies = 0
            for result in thread_results:
                total_processed += result['processed']
                total_excluded += result.get('excluded', 0)
                total_asset_calls += result['total_asset_calls']
                total_successful_asset_calls += result['successful_asset_calls']
                total_failed_asset_calls += result['failed_asset_calls']
                total_sql_based_policies += result['sqlBasedPolicies']
                total_column_based_policies += result['columnBasedPolicies']
                excluded_info = f", {result.get('excluded', 0)} excluded" if existing_target_assets_mode else ""
                print(f"Thread {result['thread_id']}: {result['processed']} policies{excluded_info}, "
                      f"{result['successful_asset_calls']}/{result['total_asset_calls']} asset calls successful")
            
            print(f"\nTotal rules exported: {total_processed}")
            if existing_target_assets_mode:
                print(f"Policies excluded (no matching assets): {total_excluded}")
            print(f"Total asset API calls made: {total_asset_calls}")
            print(f"Successful asset calls: {total_successful_asset_calls}")
            print(f"Failed asset calls: {total_failed_asset_calls}")
            print(f"SQL based policies: {total_sql_based_policies}")
            print(f"Column based policies: {total_column_based_policies}")
            print("="*80)
        
        # After all threads complete, aggregate excluded_policies_and_table_assets_mapping and write to CSV
        excluded_policies_all = []
        for thread_result in thread_results:
            if 'excluded_policies_and_table_assets_mapping' in thread_result:
                excluded_policies_all.extend(thread_result['excluded_policies_and_table_assets_mapping'])
        if excluded_policies_all:
            # Determine output directory (reuse logic from other exports)
            if globals.GLOBAL_OUTPUT_DIR:
                output_dir = globals.GLOBAL_OUTPUT_DIR / "policy-export"
            else:
                current_dir = Path.cwd()
                toolkit_dirs = [d for d in current_dir.iterdir() if d.is_dir() and d.name.startswith("adoc-migration-toolkit-")]
                if toolkit_dirs:
                    toolkit_dirs.sort(key=lambda x: x.stat().st_ctime, reverse=True)
                    latest_toolkit_dir = toolkit_dirs[0]
                    output_dir = latest_toolkit_dir / "policy-export"
                else:
                    output_dir = Path("policy-export")
            output_dir.mkdir(parents=True, exist_ok=True)
            excluded_sql_view_policiesfile = output_dir / "excluded_sql_view_policies.csv"
            with open(excluded_sql_view_policiesfile, 'w', newline='', encoding='utf-8') as f:
                writer = csv.writer(f, quoting=csv.QUOTE_ALL)
                writer.writerow(["asset_id", "source_asset_uid", "policy_id", "policy_name"])
                for asset_id, source_asset_uid, policy_id, policy_name in excluded_policies_all:
                    writer.writerow([asset_id, source_asset_uid, policy_id, policy_name])
            if not quiet_mode:
                print(f"\nExcluded policies written to: {excluded_sql_view_policiesfile}")
        
    except Exception as e:
        error_msg = f"Failed to execute parallel policy list export: {e}"
        print(f"‚ùå {error_msg}")
        logger.error(error_msg)
        raise


def execute_policy_export(client, logger: logging.Logger, quiet_mode: bool = False, verbose_mode: bool = False, batch_size: int = 50, export_type: str = None, filter_value: str = None, filter_versions: bool = True):
    """Execute the policy-export command.
    
    Args:
        client: API client instance
        logger: Logger instance
        quiet_mode: Whether to suppress console output
        verbose_mode: Whether to enable verbose logging
        batch_size: Number of policies to export in each batch
        export_type: Type of export (rule-types, engine-types, assemblies, source-types)
        filter_value: Optional filter value within the export type
        filter_versions: Whether to filter policy versions
    """
    try:
        # Determine input and output file paths
        if globals.GLOBAL_OUTPUT_DIR:
            input_file = globals.GLOBAL_OUTPUT_DIR / "policy-export" / "policies-all-export.csv"
            output_dir = globals.GLOBAL_OUTPUT_DIR / "policy-export"
        else:
            # Use the same logic as policy-list-export to find the input file
            # Look for the most recent adoc-migration-toolkit-YYYYMMDDHHMM directory
            current_dir = Path.cwd()
            toolkit_dirs = list(current_dir.glob("adoc-migration-toolkit-*"))
            
            if not toolkit_dirs:
                error_msg = "No adoc-migration-toolkit directory found. Please run 'policy-list-export' first."
                print(f"‚ùå {error_msg}")
                logger.error(error_msg)
                return
            
            # Sort by creation time and use the most recent
            toolkit_dirs.sort(key=lambda x: x.stat().st_ctime, reverse=True)
            latest_toolkit_dir = toolkit_dirs[0]
            
            input_file = latest_toolkit_dir / "policy-export" / "policies-all-export.csv"
            output_dir = latest_toolkit_dir / "policy-export"
        
        if not quiet_mode:
            print(f"\nExporting policy definitions by type")
            print(f"Input file: {input_file}")
            print(f"Output directory: {output_dir}")
            if verbose_mode:
                print("üîä VERBOSE MODE - Detailed output including headers and responses")
            print("="*80)
        
        # Check if input file exists
        if not input_file.exists():
            error_msg = f"Input file does not exist: {input_file}"
            print(f"‚ùå {error_msg}")
            print(f"üí° Please run 'policy-list-export' first to generate the input file")
            logger.error(error_msg)
            return
        
        # Read policies from CSV file
        if not quiet_mode:
            print("Reading policies from CSV file...")
        
        policies_by_category = {}
        total_policies = 0
        
        with open(input_file, 'r', newline='', encoding='utf-8') as f:
            reader = csv.reader(f)
            header = next(reader)  # Skip header
            
            # Check if this is the new format with additional columns
            if len(header) >= 7 and header[0] == 'id' and header[1] == 'type' and header[2] == 'engineType':
                # New format with additional columns
                expected_columns = ['id', 'type', 'engineType', 'tableAssetIds', 'assemblyIds', 'assemblyNames', 'sourceTypes', 'subType', 'policyName', 'tableAssetIdsTypes']
                if len(header) != len(expected_columns):
                    error_msg = f"Invalid CSV format. Expected {len(expected_columns)} columns, got {len(header)}"
                    print(f"‚ùå {error_msg}")
                    logger.error(error_msg)
                    return
            elif len(header) == 3 and header[0] == 'id' and header[1] == 'type' and header[2] == 'engineType':
                # Old format - only basic columns
                error_msg = "CSV file is in old format. Please run 'policy-list-export' first to generate the new format with additional columns."
                print(f"‚ùå {error_msg}")
                logger.error(error_msg)
                return
            else:
                error_msg = f"Invalid CSV format. Expected header: ['id', 'type', 'engineType', ...], got: {header}"
                print(f"‚ùå {error_msg}")
                logger.error(error_msg)
                return
            
            for row_num, row in enumerate(reader, start=2):
                if len(row) != len(header):
                    logger.warning(f"Row {row_num}: Expected {len(header)} columns, got {len(row)}")
                    continue
                
                policy_id = row[0].strip()
                policy_type = row[1].strip()
                engine_type = row[2].strip()
                table_asset_ids = row[3].strip()
                assembly_ids = row[4].strip()
                assembly_names = row[5].strip()
                source_types = row[6].strip()
                
                if not policy_id:
                    logger.warning(f"Row {row_num}: Empty policy ID")
                    continue
                
                # Determine the category based on export_type
                category = None
                category_value = None
                
                if export_type == 'rule-types':
                    category = policy_type
                    category_value = policy_type
                elif export_type == 'engine-types':
                    category = engine_type
                    category_value = engine_type
                elif export_type == 'assemblies':
                    if assembly_names:
                        # Split by comma and use the first assembly name
                        assembly_name_list = [name.strip() for name in assembly_names.split(',') if name.strip()]
                        if assembly_name_list:
                            category = assembly_name_list[0]
                            category_value = assembly_names
                elif export_type == 'source-types':
                    if source_types:
                        # Split by comma and use the first source type
                        source_type_list = [stype.strip() for stype in source_types.split(',') if stype.strip()]
                        if source_type_list:
                            category = source_type_list[0]
                            category_value = source_types
                else:
                    # Default: group by policy type (original behavior)
                    category = policy_type
                    category_value = policy_type
                
                # Apply filter if specified
                if filter_value and category != filter_value:
                    continue
                
                if category:
                    if category not in policies_by_category:
                        policies_by_category[category] = []
                    policies_by_category[category].append(policy_id)
                    total_policies += 1
                else:
                    logger.warning(f"Row {row_num}: No valid category found for export type '{export_type}'")
        
        if not policies_by_category:
            error_msg = "No valid policies found matching the specified criteria"
            print(f"‚ùå {error_msg}")
            logger.error(error_msg)
            return
        
        # Determine output filename based on export type and filter
        if export_type:
            base_filename = export_type.replace('-', '_')
            if filter_value:
                # Sanitize filter value for filename
                safe_filter = "".join(c for c in filter_value if c.isalnum() or c in (' ', '-', '_')).rstrip()
                safe_filter = safe_filter.replace(' ', '_').lower()
                filename = f"{base_filename}_{safe_filter}"
            else:
                filename = base_filename
        else:
            filename = "policy_types"
        
        if not quiet_mode:
            export_type_display = export_type if export_type else "policy types"
            filter_display = f" (filtered by: {filter_value})" if filter_value else ""
            print(f"Found {total_policies} policies across {len(policies_by_category)} {export_type_display}{filter_display}")
            print(f"Output filename base: {filename}")
            print("="*80)
        
        # Generate timestamp for all files
        timestamp = datetime.now().strftime("%m-%d-%Y-%H-%M")
        
        # Export policies by type in batches
        successful_exports = 0
        failed_exports = 0
        export_results = {}
        
        # Calculate total batches for progress bar
        total_batches = 0
        for policy_ids in policies_by_category.values():
            total_batches += (len(policy_ids) + batch_size - 1) // batch_size
        
        current_batch = 0
        failed_batch_indices = set()
        
        # Print initial progress bar and status line
        if not quiet_mode:
            print(f"Exporting: [{'‚ñë' * 50}] 0/{total_batches} (0.0%)")
            print(f"Status: Initializing...")
        
        batch_idx = 0
        for policy_type, policy_ids in policies_by_category.items():
            if not quiet_mode:
                # Update status line for new policy type (but don't reset progress bar)
                # Calculate current progress
                percentage = (current_batch / total_batches) * 100
                bar_width = 50
                filled_blocks = int((current_batch / total_batches) * bar_width)
                
                # Build the current bar state
                bar = ''
                for i in range(bar_width):
                    if i < filled_blocks:
                        batch_index_for_block = int((i / bar_width) * total_batches)
                        if batch_index_for_block in failed_batch_indices:
                            bar += '\033[31m‚ñà\033[0m'  # Red for failed
                        else:
                            bar += '\033[32m‚ñà\033[0m'  # Green for success
                    else:
                        bar += '‚ñë'  # Empty block
                
                print(f"\033[2F\033[KExporting: [{bar}] {current_batch}/{total_batches} ({percentage:.1f}%)")
                print(f"\033[KStatus: Processing {policy_type} ({len(policy_ids)} policies)")
            else:
                print(f"Processing {policy_type}: {len(policy_ids)} policies")
            
            type_total_batches = (len(policy_ids) + batch_size - 1) // batch_size
            for batch_num in range(type_total_batches):
                start_idx = batch_num * batch_size
                end_idx = min((batch_num + 1) * batch_size, len(policy_ids))
                batch_ids = policy_ids[start_idx:end_idx]
                current_batch += 1
                
                # Generate filename with range information
                # Use the actual policy type name (category) for the filename
                safe_category = "".join(c for c in policy_type if c.isalnum() or c in (' ', '-', '_')).rstrip()
                safe_category = safe_category.replace(' ', '_').lower()
                batch_filename = f"{safe_category}-{timestamp}-{start_idx}-{end_idx-1}.zip"
                output_file = output_dir / batch_filename
                
                # Prepare query parameters
                ids_param = ','.join(batch_ids)
                query_params = {
                    'ruleStatus': 'ALL',
                    'includeTags': 'true',
                    'ids': ids_param,
                    'filename': batch_filename
                }
                
                # Build endpoint with query parameters
                endpoint = "/catalog-server/api/rules/export/policy-definitions"
                query_string = '&'.join([f"{k}={v}" for k, v in query_params.items()])
                full_endpoint = f"{endpoint}?{query_string}"
                
                if verbose_mode:
                    print(f"\n" + "="*80)
                    print(f"üîç DETAILED REQUEST INFORMATION")
                    print("="*80)
                    print(f"Policy Type: {policy_type}")
                    print(f"Batch Number: {batch_num + 1}/{type_total_batches}")
                    print(f"Batch Range: {start_idx}-{end_idx-1}")
                    print(f"Policy IDs in Batch: {len(batch_ids)}")
                    print(f"Output File: {batch_filename}")
                    print(f"Full Endpoint: {full_endpoint}")
                    print(f"Method: GET")
                    print(f"Expected Content-Type: application/zip")
                    print(f"Authorization: Bearer [REDACTED]")
                    if hasattr(client, 'tenant') and client.tenant:
                        print(f"X-Tenant: {client.tenant}")
                    print(f"\nQuery Parameters:")
                    for k, v in query_params.items():
                        if k == 'ids':
                            print(f"  {k}: {len(batch_ids)} IDs")
                            print(f"    First 5 IDs: {', '.join(batch_ids[:5])}")
                            if len(batch_ids) > 5:
                                print(f"    Last 5 IDs: {', '.join(batch_ids[-5:])}")
                        else:
                            print(f"  {k}: {v}")
                    print("="*80)
                
                try:
                    # Make API call to get ZIP file
                    response = client.make_api_call(
                        endpoint=full_endpoint,
                        method='GET',
                        return_binary=True
                    )
                    
                    if verbose_mode:
                        print(f"\n" + "="*80)
                        print(f"üì• DETAILED RESPONSE INFORMATION")
                        print("="*80)
                        print(f"Status: Success")
                        print(f"Content-Type: application/zip")
                        print(f"Response Size: {len(response) if response else 0} bytes")
                        if response:
                            print(f"File Size (KB): {len(response) / 1024:.2f} KB")
                            print(f"File Size (MB): {len(response) / (1024 * 1024):.2f} MB")
                        print(f"Response Type: {type(response).__name__}")
                        print(f"Response is Binary: {isinstance(response, bytes)}")
                        if response:
                            print(f"First 100 bytes (hex): {response[:100].hex()}")
                            print(f"Last 100 bytes (hex): {response[-100:].hex()}")
                        print("="*80)
                    
                    # Write ZIP file to output directory
                    if response:
                        with open(output_file, 'wb') as f:
                            f.write(response)
                        
                        # Filter policy versions if enabled
                        if filter_versions:
                            try:
                                success, policies_processed, versions_removed = filter_policy_versions(
                                    output_file, quiet_mode, verbose_mode
                                )
                                if success and not quiet_mode:
                                    print(f"üîß Filtered {policies_processed} SCHEMA_DRIFT policies, removed {versions_removed} older versions from {batch_filename}")
                            except Exception as filter_error:
                                if verbose_mode:
                                    print(f"‚ö†Ô∏è  Version filtering failed for {batch_filename}: {filter_error}")
                                logger.warning(f"Version filtering failed for {batch_filename}: {filter_error}")
                        
                        # Store result for this batch
                        batch_key = f"{policy_type}_batch_{batch_num + 1}"
                        export_results[batch_key] = {
                            'success': True,
                            'filename': batch_filename,
                            'count': len(batch_ids),
                            'file_size': len(response),
                            'range': f"{start_idx}-{end_idx-1}"
                        }
                        successful_exports += 1
                    else:
                        error_msg = f"Empty response for {policy_type} batch {batch_num + 1}"
                        if verbose_mode:
                            print(f"\n" + "="*80)
                            print(f"‚ùå ERROR RESPONSE INFORMATION")
                            print("="*80)
                            print(f"Error Type: Empty Response")
                            print(f"Policy Type: {policy_type}")
                            print(f"Batch Number: {batch_num + 1}")
                            print(f"Expected Content: ZIP file")
                            print(f"Actual Response: None/Empty")
                            print(f"Response Size: 0 bytes")
                            print(f"Response Type: {type(response).__name__}")
                            print("="*80)
                        logger.error(error_msg)
                        
                        # Mark this batch as failed
                        failed_batch_indices.add(batch_idx)
                        
                        batch_key = f"{policy_type}_batch_{batch_num + 1}"
                        export_results[batch_key] = {
                            'success': False,
                            'filename': batch_filename,
                            'count': len(batch_ids),
                            'error': error_msg,
                            'range': f"{start_idx}-{end_idx-1}"
                        }
                        failed_exports += 1
                        
                except Exception as e:
                    error_msg = f"Failed to export {policy_type} batch {batch_num + 1}: {e}"
                    if verbose_mode:
                        print(f"\n" + "="*80)
                        print(f"‚ùå EXCEPTION RESPONSE INFORMATION")
                        print("="*80)
                        print(f"Error Type: Exception")
                        print(f"Policy Type: {policy_type}")
                        print(f"Batch Number: {batch_num + 1}")
                        print(f"Exception Type: {type(e).__name__}")
                        print(f"Exception Message: {str(e)}")
                        print(f"Full Endpoint Attempted: {full_endpoint}")
                        print(f"Query Parameters:")
                        for k, v in query_params.items():
                            if k == 'ids':
                                print(f"  {k}: {len(batch_ids)} IDs")
                            else:
                                print(f"  {k}: {v}")
                        print("="*80)
                    logger.error(error_msg)
                    
                    # Mark this batch as failed
                    failed_batch_indices.add(batch_idx)
                    
                    batch_key = f"{policy_type}_batch_{batch_num + 1}"
                    export_results[batch_key] = {
                        'success': False,
                        'filename': batch_filename,
                        'count': len(batch_ids),
                        'error': str(e),
                        'range': f"{start_idx}-{end_idx-1}"
                    }
                    failed_exports += 1
                
                # Update progress bar and status in place
                if not quiet_mode:
                    percentage = (current_batch / total_batches) * 100
                    bar_width = 50
                    
                    # Calculate how many blocks should be filled based on current progress
                    filled_blocks = int((current_batch / total_batches) * bar_width)
                    
                    # Build the bar with exactly 50 characters
                    bar = ''
                    for i in range(bar_width):
                        if i < filled_blocks:
                            # This block should be filled - check if it's a failed batch
                            # Map the block index back to batch index
                            batch_index_for_block = int((i / bar_width) * total_batches)
                            if batch_index_for_block in failed_batch_indices:
                                bar += '\033[31m‚ñà\033[0m'  # Red for failed
                            else:
                                bar += '\033[32m‚ñà\033[0m'  # Green for success
                        else:
                            bar += '‚ñë'  # Empty block
                    
                    # Move cursor up 2 lines and update both progress bar and status
                    print(f"\033[2F\033[KExporting: [{bar}] {current_batch}/{total_batches} ({percentage:.1f}%)")
                    print(f"\033[KStatus: Processing {policy_type} batch {batch_num + 1}")
                else:
                    print(f"  Batch {batch_num + 1}/{type_total_batches}: {len(batch_ids)} policies")
                    if response:
                        print(f"‚úÖ {batch_filename}")
                    else:
                        print(f"‚ùå Failed")
                
                batch_idx += 1
        
        # Print final progress bar and status
        if not quiet_mode:
            bar_width = 50
            bar = ''
            for i in range(bar_width):
                # Map the block index back to batch index
                batch_index_for_block = int((i / bar_width) * total_batches)
                if batch_index_for_block in failed_batch_indices:
                    bar += '\033[31m‚ñà\033[0m'  # Red for failed
                else:
                    bar += '\033[32m‚ñà\033[0m'  # Green for success
            
            print(f"\033[2F\033[KExporting: [{bar}] {total_batches}/{total_batches} (100.0%)")
            print(f"\033[KStatus: Completed.")
            print()  # Add a blank line after completion
        
        # Print summary
        print("\n" + "="*80)
        print("POLICY EXPORT SUMMARY")
        print("="*80)
        print(f"Output directory: {output_dir}")
        print(f"Timestamp: {timestamp}")
        print(f"Batch size: {batch_size}")
        print(f"Total policy types processed: {len(policies_by_category)}")
        print(f"Successful exports: {successful_exports}")
        print(f"Failed exports: {failed_exports}")
        
        print(f"\nExport Results:")
        # Group results by policy type for better display
        results_by_type = {}
        for batch_key, result in export_results.items():
            policy_type = batch_key.split('_batch_')[0]
            if policy_type not in results_by_type:
                results_by_type[policy_type] = []
            results_by_type[policy_type].append(result)
        
        for policy_type, batch_results in results_by_type.items():
            print(f"  {policy_type}:")
            for result in batch_results:
                if result['success']:
                    print(f"    ‚úÖ Batch {result['range']}: {result['count']} policies -> {result['filename']} ({result['file_size']} bytes)")
                else:
                    print(f"    ‚ùå Batch {result['range']}: {result['count']} policies -> {result['error']}")
        
        print("="*80)
        
        if failed_exports > 0:
            print("‚ö†Ô∏è  Export completed with errors. Check log file for details.")
        else:
            print("‚úÖ Export completed successfully!")
            
    except Exception as e:
        error_msg = f"Error executing policy export: {e}"
        print(f"‚ùå {error_msg}")
        logger.error(error_msg)


def execute_policy_import(client, logger: logging.Logger, file_pattern: str, quiet_mode: bool = False, verbose_mode: bool = False):
    """Execute the policy-import command.
    
    Args:
        client: API client instance
        logger: Logger instance
        file_pattern: File path or glob pattern for ZIP files
        quiet_mode: Whether to suppress console output
        verbose_mode: Whether to enable verbose logging
    """
    try:
        if not quiet_mode:
            print(f"\nImporting policy definitions from ZIP files")
            print(f"File pattern: {file_pattern}")
            print("="*80)
            
            # Show target environment information
            print(f"\nüåç TARGET ENVIRONMENT INFORMATION:")
            target_host = client._build_host_url(use_target_tenant=True)
            print(f"  Host: {target_host}")
            if hasattr(client, 'target_tenant') and client.target_tenant:
                print(f"  Target Tenant: {client.target_tenant}")
            else:
                print(f"  Source Tenant: {client.tenant} (will be used as target)")
            print(f"  Authentication: Target access key and secret key")
            print("="*80)
        
        # Determine the search directory
        # If file_pattern is an absolute path, use it as is
        # Otherwise, look in the output-dir/policy-import directory
        if os.path.isabs(file_pattern):
            search_pattern = file_pattern
            search_dir = os.path.dirname(file_pattern) if os.path.dirname(file_pattern) else "."
        else:
            # Use the global output directory or default to current directory
            if globals.GLOBAL_OUTPUT_DIR:
                search_dir = globals.GLOBAL_OUTPUT_DIR / "policy-import"
            else:
                # Fallback to current directory with timestamped subdirectory
                from datetime import datetime
                timestamp = datetime.now().strftime("%Y%m%d%H%M")
                search_dir = Path(f"adoc-migration-toolkit-{timestamp}/policy-import")
            
            # Ensure the search directory exists
            if not search_dir.exists():
                error_msg = f"Policy import directory does not exist: {search_dir}"
                print(f"‚ùå {error_msg}")
                print(f"üí° Expected location: {search_dir}")
                print(f"üí° Use 'policy-export' first to generate ZIP files, or specify an absolute path")
                logger.error(error_msg)
                return
            
            search_pattern = str(search_dir / file_pattern)
        
        if not quiet_mode:
            print(f"üìÅ Searching for files in: {search_dir}")
            print(f"üîç Search pattern: {search_pattern}")
            print("="*80)
        
        # Find all matching ZIP files
        zip_files = glob(search_pattern)
        if not zip_files:
            error_msg = f"No ZIP files found matching pattern: {file_pattern}"
            print(f"‚ùå {error_msg}")
            print(f"üìÅ Searched in: {search_dir}")
            print(f"üí° Expected location: {search_dir}")
            print(f"üí° Use 'policy-export' first to generate ZIP files, or specify an absolute path")
            logger.error(error_msg)
            return
        
        # Filter to only ZIP files
        zip_files = [f for f in zip_files if f.lower().endswith('.zip')]
        if not zip_files:
            error_msg = f"No ZIP files found matching pattern: {file_pattern}"
            print(f"‚ùå {error_msg}")
            print(f"üìÅ Searched in: {search_dir}")
            print(f"üí° Expected location: {search_dir}")
            print(f"üí° Use 'policy-export' first to generate ZIP files, or specify an absolute path")
            logger.error(error_msg)
            return
        
        if not quiet_mode:
            print(f"Found {len(zip_files)} ZIP files to import")
            print("="*80)
        
        # Statistics aggregation
        aggregated_stats = {
            'conflictingAssemblies': 0,
            'conflictingPolicies': 0,
            'conflictingSqlViews': 0,
            'conflictingVisualViews': 0,
            'preChecks': 0,
            'totalAssetUDFVariablesPerAsset': 0,
            'totalBusinessRules': 0,
            'totalDataCadencePolicyCount': 0,
            'totalDataDriftPolicyCount': 0,
            'totalDataQualityPolicyCount': 0,
            'totalDataSourceCount': 0,
            'totalPolicyCount': 0,
            'totalProfileAnomalyPolicyCount': 0,
            'totalReconciliationPolicyCount': 0,
            'totalReferenceAssets': 0,
            'totalSchemaDriftPolicyCount': 0,
            'totalUDFPackages': 0,
            'totalUDFTemplates': 0,
            'files_processed': 0,
            'files_failed': 0,
            'upload_configs_successful': 0,
            'upload_configs_failed': 0,
            'apply_configs_successful': 0,
            'apply_configs_failed': 0,
            'uuids': []
        }
        
        successful_imports = 0
        failed_imports = 0
        
        # PHASE 1: Upload all ZIP files and collect UUIDs
        if not quiet_mode:
            print(f"\nüì§ PHASE 1: Uploading all ZIP files")
            print("="*80)
        
        upload_results = []  # List to store (zip_file, upload_response, upload_uuid) tuples
        
        for i, zip_file in enumerate(zip_files, 1):
            if not quiet_mode:
                print(f"Uploading file {i}/{len(zip_files)}: {zip_file}")
            
            try:
                # Validate file exists and is readable
                if not os.path.exists(zip_file):
                    error_msg = f"File does not exist: {zip_file}"
                    if not quiet_mode:
                        print(f"‚ùå {error_msg}")
                    logger.error(error_msg)
                    failed_imports += 1
                    aggregated_stats['files_failed'] += 1
                    continue
                
                if not os.path.isfile(zip_file):
                    error_msg = f"Path is not a file: {zip_file}"
                    if not quiet_mode:
                        print(f"‚ùå {error_msg}")
                    logger.error(error_msg)
                    failed_imports += 1
                    aggregated_stats['files_failed'] += 1
                    continue
                
                # Validate that it's a valid ZIP file
                try:
                    import zipfile
                    with zipfile.ZipFile(zip_file, 'r') as zip_ref:
                        # Test if the ZIP file can be read
                        file_list = zip_ref.namelist()
                        if not file_list:
                            error_msg = f"ZIP file is empty: {zip_file}"
                            if not quiet_mode:
                                print(f"‚ùå {error_msg}")
                            logger.error(error_msg)
                            failed_imports += 1
                            aggregated_stats['files_failed'] += 1
                            continue
                        
                        if verbose_mode:
                            print(f"  üì¶ ZIP file validation successful:")
                            print(f"    Files in ZIP: {len(file_list)}")
                            print(f"    Sample files: {file_list[:3]}")
                            
                except zipfile.BadZipFile:
                    error_msg = f"Invalid ZIP file: {zip_file}"
                    if not quiet_mode:
                        print(f"‚ùå {error_msg}")
                    logger.error(error_msg)
                    failed_imports += 1
                    aggregated_stats['files_failed'] += 1
                    continue
                except Exception as e:
                    error_msg = f"Error validating ZIP file {zip_file}: {e}"
                    if not quiet_mode:
                        print(f"‚ùå {error_msg}")
                    logger.error(error_msg)
                    failed_imports += 1
                    aggregated_stats['files_failed'] += 1
                    continue
                
                # Prepare the multipart form data
                # Read the file content first, then prepare the files dictionary
                with open(zip_file, 'rb') as f:
                    file_content = f.read()
                
                # Prepare files dictionary with proper format for multipart upload
                files = {
                    'policy-config-file': (
                        os.path.basename(zip_file),  # filename
                        file_content,                # file content as bytes
                        'application/zip'            # content type
                    )
                }
                
                if verbose_mode:
                    print(f"  üìÅ File Details:")
                    print(f"    File size: {len(file_content)} bytes ({len(file_content) / 1024:.2f} KB)")
                    print(f"    File name: {os.path.basename(zip_file)}")
                    print(f"    Content type: application/zip")
                    print(f"    Form field name: policy-config-file")
                
                if verbose_mode:
                    print(f"\nPOST Request Headers:")
                    print(f"  Endpoint: /catalog-server/api/rules/import/policy-definitions/upload-config")
                    print(f"  Method: POST")
                    print(f"  Content-Type: multipart/form-data")
                    print(f"  Accept: application/json")
                    print(f"  Authorization: Bearer [REDACTED] (Target credentials)")
                    if hasattr(client, 'target_tenant') and client.target_tenant:
                        print(f"  X-Tenant: {client.target_tenant} (Target tenant)")
                    else:
                        print(f"  X-Tenant: {client.tenant} (Source tenant - will be used as target)")
                    print(f"  File: {zip_file}")
                
                # Upload config
                if verbose_mode:
                    print(f"\nüì§ Upload Config")
                    print(f"  Endpoint: /catalog-server/api/rules/import/policy-definitions/upload-config")
                    print(f"  Method: POST")
                    print(f"  Content-Type: multipart/form-data")
                    print(f"  File: {zip_file}")
                
                # Add longer timeout for file uploads
                upload_response = client.make_api_call(
                    endpoint="/catalog-server/api/rules/import/policy-definitions/upload-config",
                    method='POST',
                    files=files,
                    use_target_auth=True,
                    use_target_tenant=True,
                    timeout=300  # Increase timeout for file uploads to 5 minutes
                )
                
                if not upload_response:
                    error_msg = f"Upload config failed - empty response for {zip_file}"
                    if not quiet_mode:
                        print(f"‚ùå {error_msg}")
                    logger.error(error_msg)
                    failed_imports += 1
                    aggregated_stats['files_failed'] += 1
                    aggregated_stats['upload_configs_failed'] += 1
                    continue
                
                # Extract UUID from upload response
                upload_uuid = upload_response.get('uuid')
                if not upload_uuid:
                    error_msg = f"Upload config failed - no UUID returned for {zip_file}"
                    if not quiet_mode:
                        print(f"‚ùå {error_msg}")
                    logger.error(error_msg)
                    failed_imports += 1
                    aggregated_stats['files_failed'] += 1
                    aggregated_stats['upload_configs_failed'] += 1
                    continue
                
                # Store successful upload result
                upload_results.append((zip_file, upload_response, upload_uuid))
                aggregated_stats['upload_configs_successful'] += 1
                
                if verbose_mode:
                    print(f"  ‚úÖ Upload config successful")
                    print(f"  UUID: {upload_uuid}")
                    print(f"  Total Policies: {upload_response.get('totalPolicyCount', 0)}")
                    print(f"  Data Quality Policies: {upload_response.get('totalDataQualityPolicyCount', 0)}")
                    print(f"  Data Sources: {upload_response.get('totalDataSourceCount', 0)}")
                else:
                    print(f"‚úÖ [{i}/{len(zip_files)}] {zip_file}: Upload successful (UUID: {upload_uuid})")
                    
            except Exception as e:
                error_msg = f"Failed to upload {zip_file}: {e}"
                if not quiet_mode:
                    print(f"‚ùå {error_msg}")
                    # Add more detailed error information
                    if "500" in str(e):
                        print(f"üí° This appears to be a server error (500). Possible causes:")
                        print(f"   - File format issue (ensure it's a valid ZIP file)")
                        print(f"   - File size too large")
                        print(f"   - Server temporarily unavailable")
                        print(f"   - Authentication or permission issues")
                    elif "timeout" in str(e).lower():
                        print(f"üí° Request timed out. Try increasing timeout or check network connection.")
                    elif "connection" in str(e).lower():
                        print(f"üí° Connection error. Check network connectivity and server availability.")
                logger.error(error_msg)
                failed_imports += 1
                aggregated_stats['files_failed'] += 1
        
        # PHASE 2: Apply config for each uploaded UUID
        if not quiet_mode:
            print(f"\nüì• PHASE 2: Applying config for {len(upload_results)} uploaded files")
            print("="*80)
        
        for i, (zip_file, upload_response, upload_uuid) in enumerate(upload_results, 1):
            if not quiet_mode:
                print(f"Applying config {i}/{len(upload_results)}: {zip_file} (UUID: {upload_uuid})")
            
            try:
                if verbose_mode:
                    print(f"\nüì• Apply Config")
                    print(f"  Endpoint: /catalog-server/api/rules/import/policy-definitions/apply-config")
                    print(f"  Method: POST")
                    print(f"  Content-Type: application/json")
                    print(f"  UUID: {upload_uuid}")
                
                # Prepare apply config payload
                apply_payload = {
                    "assemblyMap": {},
                    "policyOverride": True,
                    "sqlViewOverride": True,
                    "visualViewOverride": False,
                    "uuid": upload_uuid
                }
                
                apply_response = client.make_api_call(
                    endpoint="/catalog-server/api/rules/import/policy-definitions/apply-config",
                    method='POST',
                    json_payload=apply_payload,
                    use_target_auth=True,
                    use_target_tenant=True
                )
                
                if apply_response:
                    aggregated_stats['apply_configs_successful'] += 1
                    aggregated_stats['files_processed'] += 1
                    successful_imports += 1
                    
                    # Aggregate statistics from upload response
                    for key in aggregated_stats.keys():
                        if key in upload_response and isinstance(upload_response[key], (int, float)):
                            if key == 'uuids':
                                if isinstance(upload_response[key], list):
                                    aggregated_stats[key].extend(upload_response[key])
                            else:
                                aggregated_stats[key] += upload_response[key]
                    
                    # Add UUID to list
                    aggregated_stats['uuids'].append(upload_uuid)
                    
                    if not quiet_mode:
                        print(f"‚úÖ Successfully applied config: {zip_file}")
                        if verbose_mode:
                            print(f"  UUID: {upload_uuid}")
                            print(f"  Total Policies: {upload_response.get('totalPolicyCount', 0)}")
                            print(f"  Data Quality Policies: {upload_response.get('totalDataQualityPolicyCount', 0)}")
                            print(f"  Data Sources: {upload_response.get('totalDataSourceCount', 0)}")
                    else:
                        print(f"‚úÖ [{i}/{len(upload_results)}] {zip_file}: Apply successful")
                else:
                    error_msg = f"Apply config failed for {zip_file} (UUID: {upload_uuid})"
                    if not quiet_mode:
                        print(f"‚ùå {error_msg}")
                    logger.error(error_msg)
                    failed_imports += 1
                    aggregated_stats['files_failed'] += 1
                    aggregated_stats['apply_configs_failed'] += 1
                    
            except Exception as e:
                error_msg = f"Failed to apply config for {zip_file}: {e}"
                if not quiet_mode:
                    print(f"‚ùå {error_msg}")
                logger.error(error_msg)
                failed_imports += 1
                aggregated_stats['files_failed'] += 1
        
        # Print summary
        print("\n" + "="*80)
        print("POLICY IMPORT SUMMARY")
        print("="*80)
        print(f"Files processed: {successful_imports}")
        print(f"Files failed: {failed_imports}")
        print(f"Total files: {len(zip_files)}")
        print(f"Upload configs successful: {aggregated_stats['upload_configs_successful']}")
        print(f"Upload configs failed: {aggregated_stats['upload_configs_failed']}")
        print(f"Apply configs successful: {aggregated_stats['apply_configs_successful']}")
        print(f"Apply configs failed: {aggregated_stats['apply_configs_failed']}")
        
        if successful_imports > 0:
            print(f"\nüìä AGGREGATED STATISTICS")
            print("-" * 50)
            print(f"Total Policies: {aggregated_stats['totalPolicyCount']}")
            print(f"Data Quality Policies: {aggregated_stats['totalDataQualityPolicyCount']}")
            print(f"Data Sources: {aggregated_stats['totalDataSourceCount']}")
            print(f"Business Rules: {aggregated_stats['totalBusinessRules']}")
            print(f"Data Cadence Policies: {aggregated_stats['totalDataCadencePolicyCount']}")
            print(f"Data Drift Policies: {aggregated_stats['totalDataDriftPolicyCount']}")
            print(f"Profile Anomaly Policies: {aggregated_stats['totalProfileAnomalyPolicyCount']}")
            print(f"Reconciliation Policies: {aggregated_stats['totalReconciliationPolicyCount']}")
            print(f"Schema Drift Policies: {aggregated_stats['totalSchemaDriftPolicyCount']}")
            print(f"Reference Assets: {aggregated_stats['totalReferenceAssets']}")
            print(f"UDF Packages: {aggregated_stats['totalUDFPackages']}")
            print(f"UDF Templates: {aggregated_stats['totalUDFTemplates']}")
            print(f"Asset UDF Variables: {aggregated_stats['totalAssetUDFVariablesPerAsset']}")
            
            print(f"\n‚ö†Ô∏è  CONFLICTS DETECTED")
            print("-" * 30)
            print(f"Conflicting Assemblies: {aggregated_stats['conflictingAssemblies']}")
            print(f"Conflicting Policies: {aggregated_stats['conflictingPolicies']}")
            print(f"Conflicting SQL Views: {aggregated_stats['conflictingSqlViews']}")
            print(f"Conflicting Visual Views: {aggregated_stats['conflictingVisualViews']}")
            
            if aggregated_stats['uuids']:
                print(f"\nüîë IMPORTED UUIDs")
                print("-" * 20)
                for uuid in aggregated_stats['uuids']:
                    print(f"  {uuid}")
        
        print("="*80)
        
        if failed_imports > 0:
            print("‚ö†Ô∏è  Import completed with errors. Check log file for details.")
        else:
            print("‚úÖ Import completed successfully!")
            
    except Exception as e:
        error_msg = f"Error executing policy import: {e}"
        print(f"‚ùå {error_msg}")
        logger.error(error_msg)


def execute_rule_tag_export(client, logger: logging.Logger, quiet_mode: bool = False, verbose_mode: bool = False):
    """Execute the rule-tag-export command.
    
    Args:
        client: API client instance
        logger: Logger instance
        quiet_mode: Whether to suppress console output
        verbose_mode: Whether to enable verbose logging
    """
    try:
        # Determine output file path using the policy-export category
        output_file = get_output_file_path("", "rule-tags-export.csv", category="policy-export")
        
        if not quiet_mode:
            print(f"\nExporting rule tags from ADOC environment")
            print(f"Output will be written to: {output_file}")
            if verbose_mode:
                print("üîä VERBOSE MODE - Detailed output including headers and responses")
            print("="*80)
        
        # Check if policies-all-export.csv exists
        if globals.GLOBAL_OUTPUT_DIR:
            policies_file = globals.GLOBAL_OUTPUT_DIR / "policy-export" / "policies-all-export.csv"
        else:
            # Look for the most recent adoc-migration-toolkit-YYYYMMDDHHMM directory
            current_dir = Path.cwd()
            toolkit_dirs = list(current_dir.glob("adoc-migration-toolkit-*"))
            
            if not toolkit_dirs:
                error_msg = "No adoc-migration-toolkit directory found. Please run 'policy-list-export' first."
                print(f"‚ùå {error_msg}")
                logger.error(error_msg)
                return
            
            # Sort by creation time and use the most recent
            toolkit_dirs.sort(key=lambda x: x.stat().st_ctime, reverse=True)
            latest_toolkit_dir = toolkit_dirs[0]
            policies_file = latest_toolkit_dir / "policy-export" / "policies-all-export.csv"
        
        # Check if policies file exists
        if not policies_file.exists():
            if not quiet_mode:
                print(f"‚ùå Policy list file not found: {policies_file}")
                print("üí° Running policy-list-export first to generate the required file...")
                print("="*80)
            
            # Run policy-list-export internally
            execute_policy_list_export(client, logger, quiet_mode, verbose_mode)
            
            # Check again if the file was created
            if not policies_file.exists():
                error_msg = "Failed to generate policies-all-export.csv file"
                print(f"‚ùå {error_msg}")
                logger.error(error_msg)
                return
        
        # Read policies from CSV file
        if not quiet_mode:
            print(f"Reading policies from: {policies_file}")
        
        rule_ids = []
        try:
            with open(policies_file, 'r', newline='', encoding='utf-8') as f:
                reader = csv.reader(f)
                header = next(reader)  # Skip header
                
                for row in reader:
                    if len(row) > 0 and row[0].strip():  # First column should be the rule ID
                        try:
                            rule_id = int(row[0].strip())
                            rule_ids.append(rule_id)
                        except ValueError:
                            # Skip non-numeric IDs
                            continue
        except Exception as e:
            error_msg = f"Failed to read policies file: {e}"
            print(f"‚ùå {error_msg}")
            logger.error(error_msg)
            return
        
        if not rule_ids:
            error_msg = "No valid rule IDs found in policies file"
            print(f"‚ùå {error_msg}")
            logger.error(error_msg)
            return
        
        if not quiet_mode:
            print(f"Found {len(rule_ids)} rules to process")
            print("="*80)
        
        # Create output directory if needed
        output_file.parent.mkdir(parents=True, exist_ok=True)
        
        # Process each rule to get tags
        successful_calls = 0
        failed_calls = 0
        rule_tags_data = []
        
        # Create progress bar
        progress_bar = create_progress_bar(
            total=len(rule_ids),
            desc="Exporting rule tags",
            unit="rules",
            disable=verbose_mode
        )
        
        for i, rule_id in enumerate(rule_ids, 1):
            # Update progress bar with current rule ID using set_postfix
            progress_bar.set_postfix(rule_id=rule_id)
            
            try:
                if verbose_mode:
                    print(f"\n\n[Rule {i}/{len(rule_ids)}] Processing rule ID: {rule_id}")
                    print(f"\nGET Request Headers:")
                    print(f"  Endpoint: /catalog-server/api/rules/{rule_id}/tags")
                    print(f"  Method: GET")
                    print(f"  Content-Type: application/json")
                    print(f"  Authorization: Bearer [REDACTED]")
                    if hasattr(client, 'tenant') and client.tenant:
                        print(f"  X-Tenant: {client.tenant}")
                
                # Make API call to get tags for this rule
                response = client.make_api_call(
                    endpoint=f"/catalog-server/api/rules/{rule_id}/tags",
                    method='GET'
                )
                
                if verbose_mode:
                    print(f"\nResponse:")
                    print(json.dumps(response, indent=2, ensure_ascii=False))
                
                # Extract tag names from response
                tag_names = []
                if response and 'ruleTags' in response:
                    for tag in response['ruleTags']:
                        tag_name = tag.get('name')
                        if tag_name:
                            tag_names.append(tag_name)
                
                # Only store the data if there are tags
                if tag_names:
                    rule_tags_data.append({
                        'rule_id': rule_id,
                        'tag_names': tag_names
                    })
                
                successful_calls += 1
                
                # Update progress bar
                progress_bar.update(1)
                
                if verbose_mode:
                    print(f"‚úÖ Rule {rule_id}: Found {len(tag_names)} tags")
                    if tag_names:
                        print(f"   Tags: {', '.join(tag_names)}")
                    else:
                        print(f"   No tags found - skipping output")
                
            except Exception as e:
                error_msg = f"Failed to get tags for rule {rule_id}: {e}"
                if verbose_mode:
                    print(f"‚ùå {error_msg}")
                logger.error(error_msg)
                failed_calls += 1
                
                # Don't store rules that failed to retrieve tags
                # Update progress bar
                progress_bar.update(1)
        
        # Close progress bar
        progress_bar.close()
        
        # Write results to CSV file
        with open(output_file, 'w', newline='', encoding='utf-8') as f:
            writer = csv.writer(f, quoting=csv.QUOTE_ALL)
            
            # Write header
            writer.writerow(['rule_id', 'tags'])
            
            # Write data
            for data in rule_tags_data:
                rule_id = data['rule_id']
                tag_names = data['tag_names']
                tags_str = ','.join(tag_names) if tag_names else ''
                writer.writerow([rule_id, tags_str])
        
        # Print statistics
        if not quiet_mode:
            print("\n" + "="*80)
            print("RULE TAG EXPORT COMPLETED")
            print("="*80)
            print(f"Output file: {output_file}")
            print(f"Total rules processed: {len(rule_ids)}")
            print(f"Successful API calls: {successful_calls}")
            print(f"Failed API calls: {failed_calls}")
            
            # Calculate statistics
            rules_with_tags = len(rule_tags_data)  # All stored rules have tags
            rules_without_tags = len(rule_ids) - rules_with_tags
            
            print(f"Rules with tags (written to output): {rules_with_tags}")
            print(f"Rules without tags (skipped): {rules_without_tags}")
            
            # Calculate success rate
            if len(rule_ids) > 0:
                success_rate = (successful_calls / len(rule_ids)) * 100
                print(f"API success rate: {success_rate:.1f}%")
            
            # Show tag statistics
            all_tags = []
            for data in rule_tags_data:
                all_tags.extend(data['tag_names'])
            
            if all_tags:
                tag_counts = {}
                for tag in all_tags:
                    tag_counts[tag] = tag_counts.get(tag, 0) + 1
                
                print(f"\nüìä TAG STATISTICS")
                print("-" * 50)
                print(f"Total unique tags: {len(tag_counts)}")
                print(f"Total tag occurrences: {len(all_tags)}")
                
                # Show top 10 most common tags
                if tag_counts:
                    print(f"\nüè∑Ô∏è  TOP 10 MOST COMMON TAGS:")
                    print("-" * 40)
                    sorted_tags = sorted(tag_counts.items(), key=lambda x: x[1], reverse=True)
                    for tag_name, count in sorted_tags[:10]:
                        percentage = (count / len(rule_ids)) * 100
                        print(f"  {tag_name:<30} {count:>5} rules ({percentage:>5.1f}%)")
            else:
                print(f"\nüìä TAG STATISTICS")
                print("-" * 50)
                print("No tags found in any rules")
            
            print("="*80)
        else:
            print(f"‚úÖ Rule tag export completed: {len(rule_ids)} rules processed, {len(rule_tags_data)} rules with tags written to output")
        
    except Exception as e:
        error_msg = f"Error in rule-tag-export: {e}"
        if not quiet_mode:
            print(f"‚ùå {error_msg}")
        logger.error(error_msg)


def execute_policy_export_parallel(client, logger: logging.Logger, quiet_mode: bool = False, verbose_mode: bool = False, batch_size: int = 50, export_type: str = None, filter_value: str = None, max_threads: int = 5, filter_versions: bool = True):
    """Execute the policy-export command with parallel processing.
    
    Args:
        client: API client instance
        logger: Logger instance
        quiet_mode: Whether to suppress console output
        verbose_mode: Whether to enable verbose logging
        batch_size: Number of policies to export in each batch
        export_type: Type of export (rule-types, engine-types, assemblies, source-types)
        filter_value: Optional filter value within the export type
        max_threads: Maximum number of threads to use (default: 5)
        filter_versions: Whether to filter policy versions to keep only the latest (default: True)
    """
    try:
        # Determine input and output file paths
        if globals.GLOBAL_OUTPUT_DIR:
            input_file = globals.GLOBAL_OUTPUT_DIR / "policy-export" / "policies-all-export.csv"
            output_dir = globals.GLOBAL_OUTPUT_DIR / "policy-export"
        else:
            # Use the same logic as policy-list-export to find the input file
            # Look for the most recent adoc-migration-toolkit-YYYYMMDDHHMM directory
            current_dir = Path.cwd()
            toolkit_dirs = list(current_dir.glob("adoc-migration-toolkit-*"))
            
            if not toolkit_dirs:
                error_msg = "No adoc-migration-toolkit directory found. Please run 'policy-list-export' first."
                print(f"‚ùå {error_msg}")
                logger.error(error_msg)
                return
            
            # Sort by creation time and use the most recent
            toolkit_dirs.sort(key=lambda x: x.stat().st_ctime, reverse=True)
            latest_toolkit_dir = toolkit_dirs[0]
            
            input_file = latest_toolkit_dir / "policy-export" / "policies-all-export.csv"
            output_dir = latest_toolkit_dir / "policy-export"
        
        if not quiet_mode:
            print(f"\nExporting policy definitions by type (Parallel Mode)")
            print(f"Input file: {input_file}")
            print(f"Output directory: {output_dir}")
            if verbose_mode:
                print("üîä VERBOSE MODE - Detailed output including headers and responses")
            print("="*80)
        
        # Check if input file exists
        if not input_file.exists():
            error_msg = f"Input file does not exist: {input_file}"
            print(f"‚ùå {error_msg}")
            print(f"üí° Please run 'policy-list-export' first to generate the input file")
            logger.error(error_msg)
            return
        
        # Read policies from CSV file
        if not quiet_mode:
            print("Reading policies from CSV file...")
        
        policies_by_category = {}
        total_policies = 0
        
        with open(input_file, 'r', newline='', encoding='utf-8') as f:
            reader = csv.reader(f)
            header = next(reader)  # Skip header
            
            # Check if this is the new format with additional columns
            if len(header) >= 7 and header[0] == 'id' and header[1] == 'type' and header[2] == 'engineType':
                # New format with additional columns
                expected_columns = ['id', 'type', 'engineType', 'tableAssetIds', 'assemblyIds', 'assemblyNames', 'sourceTypes', 'subType', 'policyName', 'tableAssetIdsTypes']
                if len(header) != len(expected_columns):
                    error_msg = f"Invalid CSV format.. Expected {len(expected_columns)} columns, got {len(header)}"
                    print(f"‚ùå {error_msg}")
                    logger.error(error_msg)
                    return
            elif len(header) == 3 and header[0] == 'id' and header[1] == 'type' and header[2] == 'engineType':
                # Old format - only basic columns
                error_msg = "CSV file is in old format. Please run 'policy-list-export' first to generate the new format with additional columns."
                print(f"‚ùå {error_msg}")
                logger.error(error_msg)
                return
            else:
                error_msg = f"Invalid CSV format. Expected header: ['id', 'type', 'engineType', ...], got: {header}"
                print(f"‚ùå {error_msg}")
                logger.error(error_msg)
                return
            
            for row_num, row in enumerate(reader, start=2):
                if len(row) != len(header):
                    logger.warning(f"Row {row_num}: Expected {len(header)} columns, got {len(row)}")
                    continue
                
                policy_id = row[0].strip()
                policy_type = row[1].strip()
                engine_type = row[2].strip()
                table_asset_ids = row[3].strip()
                assembly_ids = row[4].strip()
                assembly_names = row[5].strip()
                source_types = row[6].strip()
                
                if not policy_id:
                    logger.warning(f"Row {row_num}: Empty policy ID")
                    continue
                
                # Determine the category based on export_type
                category = None
                category_value = None
                
                if export_type == 'rule-types':
                    category = policy_type
                    category_value = policy_type
                elif export_type == 'engine-types':
                    category = engine_type
                    category_value = engine_type
                elif export_type == 'assemblies':
                    if assembly_names:
                        # Split by comma and use the first assembly name
                        assembly_name_list = [name.strip() for name in assembly_names.split(',') if name.strip()]
                        if assembly_name_list:
                            category = assembly_name_list[0]
                            category_value = assembly_names
                elif export_type == 'source-types':
                    if source_types:
                        # Split by comma and use the first source type
                        source_type_list = [stype.strip() for stype in source_types.split(',') if stype.strip()]
                        if source_type_list:
                            category = source_type_list[0]
                            category_value = source_types
                else:
                    # Default: group by policy type (original behavior)
                    category = policy_type
                    category_value = policy_type
                
                # Apply filter if specified
                if filter_value and category != filter_value:
                    continue
                
                if category:
                    if category not in policies_by_category:
                        policies_by_category[category] = []
                    policies_by_category[category].append(policy_id)
                    total_policies += 1
                else:
                    logger.warning(f"Row {row_num}: No valid category found for export type '{export_type}'")
        
        if not policies_by_category:
            error_msg = "No valid policies found matching the specified criteria"
            print(f"‚ùå {error_msg}")
            logger.error(error_msg)
            return
        
        # Determine output filename based on export type and filter
        if export_type:
            base_filename = export_type.replace('-', '_')
            if filter_value:
                # Sanitize filter value for filename
                safe_filter = "".join(c for c in filter_value if c.isalnum() or c in (' ', '-', '_')).rstrip()
                safe_filter = safe_filter.replace(' ', '_').lower()
                filename = f"{base_filename}_{safe_filter}"
            else:
                filename = base_filename
        else:
            filename = "policy_types"
        
        if not quiet_mode:
            export_type_display = export_type if export_type else "policy types"
            filter_display = f" (filtered by: {filter_value})" if filter_value else ""
            print(f"Found {total_policies} policies across {len(policies_by_category)} {export_type_display}{filter_display}")
            print(f"Output filename base: {filename}")
            print("="*80)
        
        # Generate timestamp for all files
        timestamp = datetime.now().strftime("%m-%d-%Y-%H-%M")
        
        # Step 3: Calculate thread configuration
        min_categories_per_thread = 1
        if len(policies_by_category) < min_categories_per_thread:
            num_threads = 1
            categories_per_thread = len(policies_by_category)
        else:
            num_threads = min(max_threads, (len(policies_by_category) + min_categories_per_thread - 1) // min_categories_per_thread)
            categories_per_thread = (len(policies_by_category) + num_threads - 1) // num_threads
        
        if not quiet_mode:
            print(f"Using {num_threads} threads to process {len(policies_by_category)} categories")
            print(f"Categories per thread: {categories_per_thread}")
            print("="*80)
        
        # Step 4: Process categories in parallel
        thread_results = []
        
        # Funny thread names for progress indicators (all same length)
        thread_names = get_thread_names()
        
        def process_category_chunk(thread_id, start_index, end_index):
            """Process a chunk of categories for a specific thread."""
            # Create a thread-local client instance
            thread_client = type(client)(
                host=client.host,
                access_key=client.access_key,
                secret_key=client.secret_key,
                tenant=getattr(client, 'tenant', None)
            )
            
            # Get categories for this thread
            category_items = list(policies_by_category.items())[start_index:end_index]
            
            # Create progress bar for this thread
            total_batches = 0
            for _, policy_ids in category_items:
                total_batches += (len(policy_ids) + batch_size - 1) // batch_size
            thread_names = get_thread_names()
            thread_name = thread_names[thread_id] if thread_id < len(thread_names) else f"Thread {thread_id}"
            progress_bar = create_progress_bar(
                total=total_batches,
                desc=thread_name,
                unit="batches",
                disable=quiet_mode,
                position=thread_id,
                leave=False
            )
            
            successful_exports = 0
            failed_exports = 0
            export_results = {}
            
            # Process each category in this thread's range
            for policy_type, policy_ids in category_items:
                type_total_batches = (len(policy_ids) + batch_size - 1) // batch_size
                
                for batch_num in range(type_total_batches):
                    start_idx = batch_num * batch_size
                    end_idx = min((batch_num + 1) * batch_size, len(policy_ids))
                    batch_ids = policy_ids[start_idx:end_idx]
                    
                    # Generate filename with range information
                    safe_category = "".join(c for c in policy_type if c.isalnum() or c in (' ', '-', '_')).rstrip()
                    safe_category = safe_category.replace(' ', '_').lower()
                    batch_filename = f"{safe_category}-{timestamp}-{start_idx}-{end_idx-1}.zip"
                    output_file = output_dir / batch_filename
                    
                    # Prepare query parameters
                    ids_param = ','.join(batch_ids)
                    query_params = {
                        'ruleStatus': 'ALL',
                        'includeTags': 'true',
                        'ids': ids_param,
                        'filename': batch_filename
                    }
                    
                    # Build endpoint with query parameters
                    endpoint = "/catalog-server/api/rules/export/policy-definitions"
                    query_string = '&'.join([f"{k}={v}" for k, v in query_params.items()])
                    full_endpoint = f"{endpoint}?{query_string}"
                    
                    if verbose_mode:
                        print(f"\n{thread_name} - GET Request Headers:")
                        print(f"  Endpoint: {full_endpoint}")
                        print(f"  Method: GET")
                        print(f"  Content-Type: application/zip")
                        print(f"  Authorization: Bearer [REDACTED]")
                        if hasattr(thread_client, 'tenant') and thread_client.tenant:
                            print(f"  X-Tenant: {thread_client.tenant}")
                        print(f"  Query Parameters:")
                        for k, v in query_params.items():
                            if k == 'ids':
                                print(f"    {k}: {len(batch_ids)} IDs (first few: {', '.join(batch_ids[:3])}{'...' if len(batch_ids) > 3 else ''})")
                            else:
                                print(f"    {k}: {v}")
                    
                    try:
                        # Make API call to get ZIP file
                        response = thread_client.make_api_call(
                            endpoint=full_endpoint,
                            method='GET',
                            return_binary=True
                        )
                        
                        if verbose_mode:
                            print(f"\n{thread_name} - Response:")
                            print(f"  Status: Success")
                            print(f"  Content-Type: application/zip")
                            print(f"  File size: {len(response) if response else 0} bytes")
                        
                        # Write ZIP file to output directory
                        if response:
                            with open(output_file, 'wb') as f:
                                f.write(response)
                            
                            # Filter policy versions if enabled
                            if filter_versions:
                                try:
                                    success, policies_processed, versions_removed = filter_policy_versions(
                                        output_file, quiet_mode, verbose_mode
                                    )
                                    if success and verbose_mode:
                                        print(f"üîß {thread_name}: Filtered {policies_processed} SCHEMA_DRIFT policies, removed {versions_removed} older versions from {batch_filename}")
                                except Exception as filter_error:
                                    if verbose_mode:
                                        print(f"‚ö†Ô∏è  {thread_name}: Version filtering failed for {batch_filename}: {filter_error}")
                                    logger.warning(f"Thread {thread_name}: Version filtering failed for {batch_filename}: {filter_error}")
                            
                            # Store result for this batch
                            batch_key = f"{policy_type}_batch_{batch_num + 1}"
                            export_results[batch_key] = {
                                'success': True,
                                'filename': batch_filename,
                                'count': len(batch_ids),
                                'file_size': len(response),
                                'range': f"{start_idx}-{end_idx-1}"
                            }
                            successful_exports += 1
                        else:
                            error_msg = f"Empty response for {policy_type} batch {batch_num + 1}"
                            if verbose_mode:
                                print(f"\n{thread_name} - ‚ùå {error_msg}")
                            logger.error(f"Thread {thread_name}: {error_msg}")
                            
                            batch_key = f"{policy_type}_batch_{batch_num + 1}"
                            export_results[batch_key] = {
                                'success': False,
                                'filename': batch_filename,
                                'count': len(batch_ids),
                                'error': error_msg,
                                'range': f"{start_idx}-{end_idx-1}"
                            }
                            failed_exports += 1
                            
                    except Exception as e:
                        error_msg = f"Failed to export {policy_type} batch {batch_num + 1}: {e}"
                        if verbose_mode:
                            print(f"\n{thread_name} - ‚ùå {error_msg}")
                        logger.error(f"Thread {thread_name}: {error_msg}")
                        
                        batch_key = f"{policy_type}_batch_{batch_num + 1}"
                        export_results[batch_key] = {
                            'success': False,
                            'filename': batch_filename,
                            'count': len(batch_ids),
                            'error': str(e),
                            'range': f"{start_idx}-{end_idx-1}"
                        }
                        failed_exports += 1
                    
                    # Update progress bar
                    progress_bar.update(1)
            
            progress_bar.close()
            
            return {
                'thread_id': thread_id,
                'successful_exports': successful_exports,
                'failed_exports': failed_exports,
                'export_results': export_results
            }
        
        # Step 5: Start threads
        threads = []
        for i in range(num_threads):
            start_index = i * categories_per_thread
            end_index = min(start_index + categories_per_thread, len(policies_by_category))
            
            thread = threading.Thread(
                target=lambda tid=i, start=start_index, end=end_index: thread_results.append(
                    process_category_chunk(tid, start, end)
                )
            )
            threads.append(thread)
            thread.start()
        
        # Step 6: Wait for all threads to complete
        for thread in threads:
            thread.join()
        
        # Step 7: Consolidate results
        total_successful_exports = 0
        total_failed_exports = 0
        all_export_results = {}
        
        for result in thread_results:
            total_successful_exports += result['successful_exports']
            total_failed_exports += result['failed_exports']
            all_export_results.update(result['export_results'])
        
        # Print summary
        print("\n" + "="*80)
        print("POLICY EXPORT SUMMARY (PARALLEL MODE)")
        print("="*80)
        print(f"Output directory: {output_dir}")
        print(f"Timestamp: {timestamp}")
        print(f"Batch size: {batch_size}")
        print(f"Total policy types processed: {len(policies_by_category)}")
        print(f"Threads used: {num_threads}")
        
        for result in thread_results:
            thread_name = thread_names[result['thread_id']] if result['thread_id'] < len(thread_names) else f"Thread {result['thread_id']}"
            print(f"{thread_name}: {result['successful_exports']} successful, {result['failed_exports']} failed")
        
        print(f"\nTotal successful exports: {total_successful_exports}")
        print(f"Total failed exports: {total_failed_exports}")
        
        print(f"\nExport Results:")
        # Group results by policy type for better display
        results_by_type = {}
        for batch_key, result in all_export_results.items():
            policy_type = batch_key.split('_batch_')[0]
            if policy_type not in results_by_type:
                results_by_type[policy_type] = []
            results_by_type[policy_type].append(result)
        
        for policy_type, batch_results in results_by_type.items():
            print(f"  {policy_type}:")
            for result in batch_results:
                if result['success']:
                    print(f"    ‚úÖ Batch {result['range']}: {result['count']} policies -> {result['filename']} ({result['file_size']} bytes)")
                else:
                    print(f"    ‚ùå Batch {result['range']}: {result['count']} policies -> {result['error']}")
        
        print("="*80)
        
        if total_failed_exports > 0:
            print("‚ö†Ô∏è  Export completed with errors. Check log file for details.")
        else:
            print("‚úÖ Export completed successfully!")
            
    except Exception as e:
        error_msg = f"Error executing parallel policy export: {e}"
        print(f"‚ùå {error_msg}")
        logger.error(error_msg)
        raise 


def execute_rule_tag_export_parallel(client, logger: logging.Logger, quiet_mode: bool = False, verbose_mode: bool = False, max_threads: int = 5):
    """Execute the rule-tag-export command with parallel processing.
    
    Args:
        client: API client instance
        logger: Logger instance
        quiet_mode: Whether to suppress console output
        verbose_mode: Whether to enable verbose logging
        max_threads: Maximum number of threads to use (default: 5)
    """
    try:
        # Determine output file path using the policy-export category
        output_file = get_output_file_path("", "rule-tags-export.csv", category="policy-export")
        
        if not quiet_mode:
            print(f"\nExporting rule tags from ADOC environment (Parallel Mode)")
            print(f"Output will be written to: {output_file}")
            if verbose_mode:
                print("üîä VERBOSE MODE - Detailed output including headers and responses")
            print("="*80)
        
        # Check if policies-all-export.csv exists
        if globals.GLOBAL_OUTPUT_DIR:
            policies_file = globals.GLOBAL_OUTPUT_DIR / "policy-export" / "policies-all-export.csv"
        else:
            # Look for the most recent adoc-migration-toolkit-YYYYMMDDHHMM directory
            current_dir = Path.cwd()
            toolkit_dirs = list(current_dir.glob("adoc-migration-toolkit-*"))
            
            if not toolkit_dirs:
                error_msg = "No adoc-migration-toolkit directory found. Please run 'policy-list-export' first."
                print(f"‚ùå {error_msg}")
                logger.error(error_msg)
                return
            
            # Sort by creation time and use the most recent
            toolkit_dirs.sort(key=lambda x: x.stat().st_ctime, reverse=True)
            latest_toolkit_dir = toolkit_dirs[0]
            policies_file = latest_toolkit_dir / "policy-export" / "policies-all-export.csv"
        
        # Check if policies file exists
        if not policies_file.exists():
            if not quiet_mode:
                print(f"‚ùå Policy list file not found: {policies_file}")
                print("üí° Running policy-list-export first to generate the required file...")
                print("="*80)
            
            # Run policy-list-export internally
            execute_policy_list_export(client, logger, quiet_mode, verbose_mode)
            
            # Check again if the file was created
            if not policies_file.exists():
                error_msg = "Failed to generate policies-all-export.csv file"
                print(f"‚ùå {error_msg}")
                logger.error(error_msg)
                return
        
        # Read policies from CSV file
        if not quiet_mode:
            print(f"Reading policies from: {policies_file}")
        
        rule_ids = []
        try:
            with open(policies_file, 'r', newline='', encoding='utf-8') as f:
                reader = csv.reader(f)
                header = next(reader)  # Skip header
                
                for row in reader:
                    if len(row) > 0 and row[0].strip():  # First column should be the rule ID
                        try:
                            rule_id = int(row[0].strip())
                            rule_ids.append(rule_id)
                        except ValueError:
                            # Skip non-numeric IDs
                            continue
        except Exception as e:
            error_msg = f"Failed to read policies file: {e}"
            print(f"‚ùå {error_msg}")
            logger.error(error_msg)
            return
        
        if not rule_ids:
            error_msg = "No valid rule IDs found in policies file"
            print(f"‚ùå {error_msg}")
            logger.error(error_msg)
            return
        
        if not quiet_mode:
            print(f"Found {len(rule_ids)} rules to process")
        
        # Step 3: Calculate thread configuration
        min_rules_per_thread = 10
        if len(rule_ids) < min_rules_per_thread:
            num_threads = 1
            rules_per_thread = len(rule_ids)
        else:
            num_threads = min(max_threads, (len(rule_ids) + min_rules_per_thread - 1) // min_rules_per_thread)
            rules_per_thread = (len(rule_ids) + num_threads - 1) // num_threads
        
        if not quiet_mode:
            print(f"Using {num_threads} threads to process {len(rule_ids)} rules")
            print(f"Rules per thread: {rules_per_thread}")
            print("="*80)
        
        # Step 4: Process rules in parallel
        thread_results = []
        temp_files = []
        
        # Funny thread names for progress indicators (all same length)
        thread_names = [
            "Rocket Thread     ",
            "Lightning Thread  ", 
            "Unicorn Thread    ",
            "Dragon Thread     ",
            "Shark Thread      "
        ]
        
        def process_rule_chunk(thread_id, start_index, end_index):
            """Process a chunk of rules for a specific thread."""
            # Create a thread-local client instance
            thread_client = type(client)(
                host=client.host,
                access_key=client.access_key,
                secret_key=client.secret_key,
                tenant=getattr(client, 'tenant', None)
            )
            
            # Get rules for this thread
            thread_rule_ids = rule_ids[start_index:end_index]
            
            # Create temporary file for this thread
            temp_file = tempfile.NamedTemporaryFile(mode='w', delete=False, suffix='.csv', encoding='utf-8')
            temp_files.append(temp_file.name)
            
            # Create progress bar for this thread
            progress_bar = create_progress_bar(
                total=len(thread_rule_ids),
                desc=thread_names[thread_id] if thread_id < len(thread_names) else f"Thread {thread_id}",
                unit="rules",
                disable=quiet_mode,
                position=thread_id,
                leave=False
            )
            
            successful_calls = 0
            failed_calls = 0
            rule_tags_data = []
            
            # Process each rule in this thread's range
            for i, rule_id in enumerate(thread_rule_ids):
                try:
                    if verbose_mode:
                        thread_name = thread_names[thread_id] if thread_id < len(thread_names) else f"Thread {thread_id}"
                        print(f"\n{thread_name} - Processing rule ID: {rule_id}")
                        print(f"GET Request Headers:")
                        print(f"  Endpoint: /catalog-server/api/rules/{rule_id}/tags")
                        print(f"  Method: GET")
                        print(f"  Content-Type: application/json")
                        print(f"  Authorization: Bearer [REDACTED]")
                        if hasattr(thread_client, 'tenant') and thread_client.tenant:
                            print(f"  X-Tenant: {thread_client.tenant}")
                    
                    # Make API call to get tags for this rule
                    response = thread_client.make_api_call(
                        endpoint=f"/catalog-server/api/rules/{rule_id}/tags",
                        method='GET'
                    )
                    
                    if verbose_mode:
                        thread_name = thread_names[thread_id] if thread_id < len(thread_names) else f"Thread {thread_id}"
                        print(f"\n{thread_name} - Response:")
                        print(json.dumps(response, indent=2, ensure_ascii=False))
                    
                    # Extract tag names from response
                    tag_names = []
                    if response and 'ruleTags' in response:
                        for tag in response['ruleTags']:
                            tag_name = tag.get('name')
                            if tag_name:
                                tag_names.append(tag_name)
                    
                    # Only store the data if there are tags
                    if tag_names:
                        rule_tags_data.append({
                            'rule_id': rule_id,
                            'tag_names': tag_names
                        })
                    
                    successful_calls += 1
                    
                    if verbose_mode:
                        thread_name = thread_names[thread_id] if thread_id < len(thread_names) else f"Thread {thread_id}"
                        print(f"{thread_name} - ‚úÖ Rule {rule_id}: Found {len(tag_names)} tags")
                        if tag_names:
                            print(f"   Tags: {', '.join(tag_names)}")
                        else:
                            print(f"   No tags found - skipping output")
                    
                except Exception as e:
                    error_msg = f"Failed to get tags for rule {rule_id}: {e}"
                    if verbose_mode:
                        thread_name = thread_names[thread_id] if thread_id < len(thread_names) else f"Thread {thread_id}"
                        print(f"\n{thread_name} - ‚ùå {error_msg}")
                    logger.error(f"Thread {thread_id}: {error_msg}")
                    failed_calls += 1
                
                # Update progress bar
                progress_bar.update(1)
            
            progress_bar.close()
            
            # Write results to temporary CSV file
            with open(temp_file.name, 'w', newline='', encoding='utf-8') as f:
                writer = csv.writer(f, quoting=csv.QUOTE_ALL)
                
                # Write header
                writer.writerow(['rule_id', 'tags'])
                
                # Write data
                for data in rule_tags_data:
                    rule_id = data['rule_id']
                    tag_names = data['tag_names']
                    tags_str = ','.join(tag_names) if tag_names else ''
                    writer.writerow([rule_id, tags_str])
            
            return {
                'thread_id': thread_id,
                'successful_calls': successful_calls,
                'failed_calls': failed_calls,
                'rules_with_tags': len(rule_tags_data),
                'temp_file': temp_file.name
            }
        
        # Step 5: Start threads
        threads = []
        for i in range(num_threads):
            start_index = i * rules_per_thread
            end_index = min(start_index + rules_per_thread, len(rule_ids))
            
            thread = threading.Thread(
                target=lambda tid=i, start=start_index, end=end_index: thread_results.append(
                    process_rule_chunk(tid, start, end)
                )
            )
            threads.append(thread)
            thread.start()
        
        # Step 6: Wait for all threads to complete
        for thread in threads:
            thread.join()
        
        # Step 7: Merge temporary files
        if not quiet_mode:
            print("\nMerging temporary files...")
        
        # Create output directory if needed
        output_file.parent.mkdir(parents=True, exist_ok=True)
        
        # Read all rows from temporary files
        all_rows = []
        for temp_file in temp_files:
            try:
                with open(temp_file, 'r', newline='', encoding='utf-8') as temp_csv:
                    reader = csv.reader(temp_csv)
                    next(reader)  # Skip header
                    for row in reader:
                        if len(row) >= 2:  # Ensure we have rule_id and tags
                            all_rows.append(row)
            except Exception as e:
                logger.error(f"Error reading temporary file {temp_file}: {e}")
            finally:
                # Clean up temporary file
                try:
                    os.unlink(temp_file)
                except Exception as e:
                    logger.warning(f"Could not delete temporary file {temp_file}: {e}")
        
        # Step 8: Sort rows by rule_id
        if not quiet_mode:
            print("Sorting results by rule ID...")
        
        def sort_key(row):
            rule_id = row[0] if len(row) > 0 else ''
            # Convert rule_id to int for proper numeric sorting, fallback to string
            try:
                rule_id_int = int(rule_id) if rule_id else 0
            except (ValueError, TypeError):
                rule_id_int = 0
            return rule_id_int
        
        all_rows.sort(key=sort_key)
        
        # Step 9: Write final output
        with open(output_file, 'w', newline='', encoding='utf-8') as f:
            writer = csv.writer(f, quoting=csv.QUOTE_ALL)
            
            # Write header
            writer.writerow(['rule_id', 'tags'])
            
            # Write sorted data
            writer.writerows(all_rows)
        
        # Step 10: Print statistics
        if not quiet_mode:
            print("\n" + "="*80)
            print("RULE TAG EXPORT COMPLETED (PARALLEL MODE)")
            print("="*80)
            print(f"Output file: {output_file}")
            print(f"Total rules processed: {len(rule_ids)}")
            print(f"Threads used: {num_threads}")
            
            total_successful_calls = 0
            total_failed_calls = 0
            total_rules_with_tags = 0
            
            for result in thread_results:
                thread_name = thread_names[result['thread_id']] if result['thread_id'] < len(thread_names) else f"Thread {result['thread_id']}"
                print(f"{thread_name}: {result['successful_calls']} successful, {result['failed_calls']} failed, {result['rules_with_tags']} with tags")
                total_successful_calls += result['successful_calls']
                total_failed_calls += result['failed_calls']
                total_rules_with_tags += result['rules_with_tags']
            
            print(f"\nTotal successful API calls: {total_successful_calls}")
            print(f"Total failed API calls: {total_failed_calls}")
            print(f"Rules with tags (written to output): {total_rules_with_tags}")
            print(f"Rules without tags (skipped): {len(rule_ids) - total_rules_with_tags}")
            
            # Calculate success rate
            if len(rule_ids) > 0:
                success_rate = (total_successful_calls / len(rule_ids)) * 100
                print(f"API success rate: {success_rate:.1f}%")
            
            # Show tag statistics
            all_tags = []
            for row in all_rows:
                if len(row) >= 2 and row[1].strip():
                    tags = [tag.strip() for tag in row[1].split(',') if tag.strip()]
                    all_tags.extend(tags)
            
            if all_tags:
                tag_counts = {}
                for tag in all_tags:
                    tag_counts[tag] = tag_counts.get(tag, 0) + 1
                
                print(f"\nüìä TAG STATISTICS")
                print("-" * 50)
                print(f"Total unique tags: {len(tag_counts)}")
                print(f"Total tag occurrences: {len(all_tags)}")
                
                # Show top 10 most common tags
                if tag_counts:
                    print(f"\nüè∑Ô∏è  TOP 10 MOST COMMON TAGS:")
                    print("-" * 40)
                    sorted_tags = sorted(tag_counts.items(), key=lambda x: x[1], reverse=True)
                    for tag_name, count in sorted_tags[:10]:
                        percentage = (count / len(rule_ids)) * 100
                        print(f"  {tag_name:<30} {count:>5} rules ({percentage:>5.1f}%)")
            else:
                print(f"\nüìä TAG STATISTICS")
                print("-" * 50)
                print("No tags found in any rules")
            
            print("="*80)
        else:
            print(f"‚úÖ Rule tag export completed: {len(rule_ids)} rules processed, {len(all_rows)} rules with tags written to output")
        
    except Exception as e:
        error_msg = f"Error in parallel rule-tag-export: {e}"
        if not quiet_mode:
            print(f"‚ùå {error_msg}")
        logger.error(error_msg)
        raise


def filter_policy_versions(zip_file_path: Path, quiet_mode: bool = False, verbose_mode: bool = False):
    """
    Filter policy versions in a ZIP file to keep only the latest version for SCHEMA_DRIFT policies.
    
    Args:
        zip_file_path: Path to the ZIP file containing policy definitions
        quiet_mode: Whether to suppress console output
        verbose_mode: Whether to enable verbose logging
    """
    import zipfile
    import tempfile
    import shutil
    
    try:
        if not quiet_mode:
            print(f"üîß Filtering SCHEMA_DRIFT policy versions in: {zip_file_path}")
        
        # Create a temporary directory for processing
        with tempfile.TemporaryDirectory() as temp_dir:
            temp_dir_path = Path(temp_dir)
            
            # Extract the ZIP file
            with zipfile.ZipFile(zip_file_path, 'r') as zip_ref:
                zip_ref.extractall(temp_dir_path)
            
            # Process each JSON file in the ZIP
            json_files = list(temp_dir_path.glob("*.json"))
            total_policies_processed = 0
            total_versions_removed = 0
            
            for json_file in json_files:
                if verbose_mode:
                    print(f"  Processing: {json_file.name}")
                
                try:
                    # Read the JSON file
                    with open(json_file, 'r', encoding='utf-8') as f:
                        policies = json.load(f)
                    
                    if not isinstance(policies, list):
                        if verbose_mode:
                            print(f"    Skipping {json_file.name}: not a list of policies")
                        continue
                    
                    original_policy_count = len(policies)
                    original_total_versions = sum(len(policy.get('items', [])) for policy in policies)
                    
                    # Process each policy
                    for policy in policies:
                        if 'items' not in policy or not policy['items']:
                            continue
                        
                        # Only filter SCHEMA_DRIFT policies
                        policy_type = policy.get('type', '')
                        if policy_type != 'SCHEMA_DRIFT':
                            if verbose_mode:
                                policy_name = policy.get('name', 'Unknown')
                                print(f"    Policy '{policy_name}' (type: {policy_type}): skipping - not SCHEMA_DRIFT")
                            continue
                        
                        items = policy['items']
                        if len(items) <= 1:
                            if verbose_mode:
                                policy_name = policy.get('name', 'Unknown')
                                print(f"    Policy '{policy_name}': skipping - only one version")
                            continue  # No filtering needed if only one version
                        
                        # Sort items by ruleVersion in descending order (latest first)
                        # Handle cases where ruleVersion might be missing
                        def get_rule_version(item):
                            return item.get('ruleVersion', 0)
                        
                        items.sort(key=get_rule_version, reverse=True)
                        
                        # Keep only the first item (latest version)
                        latest_item = items[0]
                        policy['items'] = [latest_item]
                        
                        if verbose_mode:
                            policy_name = policy.get('name', 'Unknown')
                            latest_version = get_rule_version(latest_item)
                            removed_count = len(items) - 1
                            print(f"    Policy '{policy_name}' (SCHEMA_DRIFT): kept version {latest_version}, removed {removed_count} older versions")
                    
                    # Write the filtered policies back to the JSON file
                    with open(json_file, 'w', encoding='utf-8') as f:
                        json.dump(policies, f, indent=2, ensure_ascii=False)
                    
                    # Calculate statistics
                    final_total_versions = sum(len(policy.get('items', [])) for policy in policies)
                    versions_removed = original_total_versions - final_total_versions
                    
                    total_policies_processed += original_policy_count
                    total_versions_removed += versions_removed
                    
                    if verbose_mode:
                        print(f"    {json_file.name}: {original_policy_count} policies, removed {versions_removed} SCHEMA_DRIFT versions")
                
                except Exception as e:
                    if verbose_mode:
                        print(f"    Error processing {json_file.name}: {e}")
                    continue
            
            # Create a new ZIP file with the filtered content
            backup_path = zip_file_path.with_suffix('.zip.backup')
            if backup_path.exists():
                backup_path.unlink()
            
            # Rename original to backup
            zip_file_path.rename(backup_path)
            
            # Create new ZIP with filtered content
            with zipfile.ZipFile(zip_file_path, 'w', zipfile.ZIP_DEFLATED) as new_zip:
                for file_path in temp_dir_path.rglob('*'):
                    if file_path.is_file():
                        arc_name = file_path.relative_to(temp_dir_path)
                        new_zip.write(file_path, arc_name)
            
            if not quiet_mode:
                print(f"‚úÖ Version filtering completed:")
                print(f"   Policies processed: {total_policies_processed}")
                print(f"   Versions removed: {total_versions_removed}")
                print(f"   Backup saved as: {backup_path}")
            
            return True, total_policies_processed, total_versions_removed
    
    except Exception as e:
        error_msg = f"Error filtering policy versions in {zip_file_path}: {e}"
        if not quiet_mode:
            print(f"‚ùå {error_msg}")
        return False, 0, 0

